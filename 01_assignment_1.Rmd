# Reproducible Research 

## Introduction {-}

During this assignment I learned to search, analyse and use different types of reproducible data.

----

## Analyse an excel file {-}

The experiment used for this analysis was obtained from an experiment in which adult C. elegans was exposed to varying concentrations of different compounds. The data for this experiment is supplied by J. Louter (INT/ILC).

The experiment contains a Excel file called CE.LIQ.FLOW.062_Tidydata.xlsx. During analysing this file I noticed that under 'compVehicle' the negative and positive seems to be alike, apart from the concentration. Negative is a higher concentration with ethanol and positive is a lower concentration with a variation of things. 
The summary of the file tells the positive control is ethanol and the negative control is S medium.

```{r opening and importing}

# Obtain file in environment
CE.LIQ.FLOW.062 <- read_excel("files/CE.LIQ.FLOW.062_Tidydata.xlsx") 

```

After import in R I checked the datatypes, but these were not assigned correctly. 
RawData is double, but should be integer, because it contains full numbers.
CompName is character, but should be factor, because the names can be changed with numbers and get sorted by.
CompConcentration is character, bu should be double, because it contains decimals.

To keep working with the file, I changed the datatypes to the correct ones. 

```{r alter type}

# Turn into integer datatype.
CE.LIQ.FLOW.062$RawData <- as.integer(CE.LIQ.FLOW.062$RawData)

# Turn into factor.
CE.LIQ.FLOW.062$compName <- as.factor(CE.LIQ.FLOW.062$compName)

# Turn into double (numeric value).
CE.LIQ.FLOW.062$compConcentration <- as.double(CE.LIQ.FLOW.062$compConcentration)

```

After this plotting was possible. The scatterplot shows the log10 of the concentration of different compounds in the C. elegans offspring count. 
The correction with log10 was necessary because the labels of the x-axis were overlapping each other, the values had a large number of decimals and were randomly positioned. 

```{r create plot}

# Obtain needed columns
CE.LIQ.FLOW.062_summary <- CE.LIQ.FLOW.062 %>% 
  group_by(compName, compConcentration, expType) %>%                          
                                 summarize(mean_counts = mean(RawData, na.rm = TRUE),
                                             sd_counts =  sd(RawData, na.rm = TRUE),)
CE.LIQ.FLOW.062_summary_tbl <- CE.LIQ.FLOW.062_summary %>% 
  filter(expType == "experiment")

# Create scatterplot
CE.LIQ.FLOW.062_summary_tbl %>% ggplot(aes(x = log10(compConcentration), 
                                           y = mean_counts)) +
  geom_point(aes(color = compName, shape = expType), size = 2, 
position = position_jitter(width = 0.5, height = 0.5, seed = 123)) +
  geom_errorbar(aes(ymin=mean_counts-sd_counts, ymax=mean_counts+sd_counts, 
                    color = compName), width= 0.2, 
                position = position_jitter(width = 0.5, height = 0.5, seed = 123))+
  labs(title = "Mean counts C. elegans offspring in various concentrations",
       x = "Log10 of compound concentration",
       y = "C. elegans offspring count") +
  theme(axis.text = element_text(size = 10)) 

```

For further analysing of the effect I would start by testing normality by performing a Shapiro-Wilk test. If unusual, I would normalize the data, if normal, I would start an ANOVA between de different conditions. I would end with the post-hoc tests, to check in what combination of groups the difference is.

In this case, the data is not normal, so normalizing is necessary.
The normalizing step is to see if the offspring count increases or decreases relatively to the negative control and if so, if there is anything influenced by the negative control.

```{r normalize data}

# Viewing CE.LIQ.FLOW.062_summary shows the mean value of controlNegative = 85.9.

# Add the column with normalized average counts
CE.LIQ.FLOW.062_summary_norm <- CE.LIQ.FLOW.062_summary_tbl %>% 
  mutate(norm_counts = mean_counts/85.9, norm_sd = sd_counts/85.9)

# Create scatterplot with normalized data
CE.LIQ.FLOW.062_summary_norm %>% ggplot(aes(x = log10(compConcentration), 
                                            y = norm_counts)) +
  geom_point(aes(color = compName, shape = expType), size = 2, 
position = position_jitter(width = 0.5, height = 0.5, seed = 150)) +
  geom_errorbar(aes(ymin=norm_counts-norm_sd, ymax=norm_counts+norm_sd, 
                    color = compName), width= 0.2, 
                position = position_jitter(width = 0.5, height = 0.5, seed = 150))+
  labs(title = "Mean counts C. elegans offspring in various concentrations",
       subtitle = "Normalized to negative control = 1",
       x = "Log10 of compound concentration",
       y = "C. elegans offspring count") +
  theme(axis.text = element_text(size = 10)) 

```

----

## Scoring reproducibility {-}

The next analysis is based on an article that addresses if meaningless novel words in linguistic contexts can achieve emotional connotations and whether these connotations can affect the quality of word learning and retention [@snefjellaHowEmotionLearned2020].

There is a lot of language research concerning the human capacity for learning new words, though this research is by far not complete yet. Because of that, the goal of this paper is to add to this research field.

To answer these questions, five experiments were set up. three groups of L1 speakers of the English language had to learn nine novel words in a consistently positive, neutral or negative context. During the learning phase, reading times were recorded. Immediately after and one week after, vocabulary post-test were administered to assess learning and retention.

The results showed that for both research questions two out of three groups learned the forms, definitional meanings en emotional connotations and that de positive group learned the best out of the three groups.

The analysis is a scoring on the basis of the [Repita transparency criteria](https://www.researchgate.net/publication/340244621_Reproducibility_and_reporting_practices_in_COVID-19_preprint_manuscripts). This is a method for identifying reproducibility issues in a scientific publication [@sumnerReproducibilityReportingPractices2020]. The scoring is shown below, the definitions can be found in the link above.

```{r repita scoring table}

# Create matrix
repita_criteria_tab <- matrix(c("Study purpose", "V", "Data availability statement", "V", "Data location", "[Available here](https://osf.io/yghx3/)", "Study location", "USA", "Author review", "[Bottom of page](https://www.sciencedirect.com/science/article/abs/pii/S0749596X20300851?via%3Dihub)", "Ethics statement", "V", "Funding statement", "V", "Code availability", "X"), ncol=2, byrow=TRUE)

# Define names of matrix
colnames(repita_criteria_tab) <- c('Transparency criteria', 'Score')

# Convert matrix to table 
repita_criteria_tab <- as.table(repita_criteria_tab)

# Design and view table 
repita_criteria_tab %>% kable(caption = "Table 1: The Repita criteria with answers concerning the article above.") %>% kable_styling(latex_options = "striped")

```

----

## Usage of other peoples codes {-}

The last analysis contains a code of an article about the case-fatality rate of COVID-19  [@dudelMonitoringTrendsDifferences2020].

Looking at the code, is seems to achieve a visualisation of the differences in case-fatality rate (CFR)  over time in between countries, using the age structure of infection and age specific CFRs. This is done by creating tables containing the latest data of diagnosed cases and death counts (concerning COVID-19) for each country.

The readability of the code is quite al right. The code in general is readable and understandable because of comments en different chuncks. But the total code is quite long and stored in different files, which makes it a little bit harder to access and run.

After trying the code myself the code was a little less accessible than I thought. At first it took a lot of effort, because the file in the url didn't exist any more and I couldn't figure out the updated pathway. The updated pathway was not clearly indicated, but once I found it the reproduction was pretty smooth and I only had to make small changes.

<details><summary>The full code is stated below, showing comments of changes i made to make it run smoothly.</summary>

```{r functions}

### Case fatality rate #######################################################

  # cc = case-age distribution
  # rr = age-specific case fatality rates
  cfr <- function(cc,rr){
    sum(cc * rr)
  }

  
### Kitagawa decomposition ####################################################

  # c1 = Age distribution population 1
  # r1 = Case fatality rates population 1
  # c2 = Age distribution population 2
  # r2 = Case fatality rates population 2
  
  kitagawa_cfr <- function(c1, r1, c2, r2){
    
    # Calculate age-distribution of cases
    c1  <- c1 / sum(c1)
    c2  <- c2 / sum(c2)
    
    # Total difference
    Tot <- cfr(c1, r1) - cfr(c2, r2)
    
    # Age component
    Aa  <- sum((c1 - c2) * (r1 + r2) / 2)
    
    # Case fatality component
    Bb  <- sum((r1 - r2) * (c1 + c2) / 2)
    
    # Output
    list(Diff = Tot, 
         AgeComp = Aa,
         RateComp = Bb, 
         CFR1 = weighted.mean(r1,c1), 
         CFR2 = weighted.mean(r2,c2))
  }
```

```{r non existing input, eval = FALSE}
## This whole chunck doesn't work anymore because the file in the url has been deleted by the owner. Instead of this file, a file in the given data, named "inputdata.csv", will be used. Because of this, the chunck will not be executed (eval = false).

  # Required packages
 source(("00_functions.R"))

  # URL + filename
  url <- 'https://osf.io/wu5ve//?action=download'
  filename <- 'Data/Output_10.csv'
  
  # Load data
  GET(url, write_disk(filename, overwrite = TRUE))
  dat <- read_csv(filename,skip=3)

  
### Edit data (select countries, etc.) ########################################
  
  # Lists of countries and regions
  countrylist <- c("China","Germany","Italy","South Korea","Spain","USA")
  region <- c("All","NYC")
  
  # Restrict
  dat <- dat %>% filter(Country %in% countrylist & Region %in% region)
  
  # Remove Tests variable
  dat <- dat %>% mutate(Tests=NULL)
  
  # Drop if no cases/Deaths
  dat <- na.omit(dat)
  
  
### Save ######################################################################
  
  write_csv(dat,path="Data/inputdata.csv")
```

```{r COVID CFR analysis, eval = FALSE}
### Load functions & packages #################################################

# Not necessary anymore, all the libraries are stored together above.
 ## source(("00_functions.R")) 


### Load and edit data ########################################################

  # Load CSV file
  dat <- read_csv("files/COVID_article/inputdata.csv")
  
  # Set Date as date
  dat$Date <- as.Date(dat$Date,"%d.%m.%y")

  # Find max dates
  maxdates <- dat %>% 
    group_by(Country,Region) %>% 
    summarize(maxdate=max(Date))
  
  # Get least common denominator
  maxdate <- maxdates %>% 
    filter(Country!="China") %>% 
    ungroup() %>% 
    summarize(min(maxdate))
  
  # As vector
  maxdate <- as.data.frame(maxdate)[1,1]


### Numbers for Table 1 #######################################################
  
  # Latest date: maxdate
  refdate <- as.Date("30.06.2020","%d.%m.%Y")
  dat2 <- dat  %>% filter(Date<=refdate) #maxdate
  
  # Aggregate case and death counts
  cases <- aggregate(Cases~Code+Date+Country+Region,data=dat2[dat2$Sex=="b",],sum) 
  deaths <- aggregate(Deaths~Code+Date+Country+Region,data=dat2[dat2$Sex=="b",],sum)
  
  # Most recent counts
  cases %>% group_by(Country,Region) %>% dplyr::slice(which.max(Date))
  deaths %>% group_by(Country,Region) %>% dplyr::slice(which.max(Date))
  
  
### Analysis for Table 2 (and appendix) #######################################
  
  # Calculate ASFRs
  dat <- dat %>% mutate(ascfr = Deaths / Cases,
                        ascfr = replace_na(ascfr, 0))
  
  # Get codes for reference countries
  maxdate <- format.Date(maxdate,"%d.%m.%Y")
  refdate <- as.Date("30.06.2020","%d.%m.%Y")
  refdate2 <- format.Date(refdate,"%d.%m.%Y")#maxdate
  
  DE_code <- paste0("DE_",refdate2)#paste0("DE_",maxdate)
  IT_code <- paste0("ITbol",refdate2)#paste0("ITinfo",maxdate)
  SK_code <- paste0("KR",refdate2)#paste0("SK",maxdate)
  
  # Decide some reference patterns (For main text: SK)
  DE <- dat %>% 
    filter(Code == DE_code,
           Sex == "b")
  IT <- dat %>% 
    filter(Code == IT_code,
           Sex == "b")
  SK <- dat %>% 
    filter(Code == SK_code,
           Sex == "b")
  
  # Decompose
  DecDE <- as.data.table(dat)[,
                              kitagawa_cfr(DE$Cases, DE$ascfr,Cases,ascfr),
                              by=list(Country, Code, Date, Sex, Region)]
  
  DecIT <- as.data.table(dat)[,
                              kitagawa_cfr(IT$Cases, IT$ascfr,Cases,ascfr),
                              by=list(Country, Code, Date, Sex,Region)]
  
  DecSK <- as.data.table(dat)[,
                              kitagawa_cfr(SK$Cases, SK$ascfr,Cases,ascfr),
                              by=list(Country, Code, Date, Sex,Region)]
  
  # Select only most recent date, both genders combined
  
  DecDE <- DecDE %>% filter(Sex=="b") %>% group_by(Country,Region) %>% filter(Date<=refdate) %>% dplyr::slice(which.max(Date))
  DecIT <- DecIT %>% filter(Sex=="b") %>% group_by(Country,Region) %>% filter(Date<=refdate) %>% dplyr::slice(which.max(Date))
  DecSK <- DecSK %>% filter(Sex=="b") %>% group_by(Country,Region) %>% filter(Date<=refdate) %>% dplyr::slice(which.max(Date))
  
  # Drop unnecessary variables
  DecDE <- DecDE %>% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp)
  DecIT <- DecIT %>% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp)
  DecSK <- DecSK %>% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp)
  
  # Calculate relative contributions
  DecDE <- DecDE %>% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecDE <- DecDE %>% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  DecIT <- DecIT %>% mutate(relAgeIT = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecIT <- DecIT %>% mutate(relRateIT = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  DecSK <- DecSK %>% mutate(relAgeSK = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecSK <- DecSK %>% mutate(relRateSK = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  # Rename
  DecDE <- DecDE %>% rename(DiffDE=Diff,AgeCompDE=AgeComp,RateCompDE=RateComp)
  DecIT <- DecIT %>% rename(DiffIT=Diff,AgeCompIT=AgeComp,RateCompIT=RateComp)
  DecSK <- DecSK %>% rename(DiffSK=Diff,AgeCompSK=AgeComp,RateCompSK=RateComp)
  
  # Sort data
  DecDE <- DecDE %>% arrange(CFR2) # Appendix
  DecIT <- DecIT %>% arrange(CFR2) # Appendix
  DecSK <- DecSK %>% arrange(CFR2) # Table 2
  
  
### Table 3: Italy trend ######################################################
  
  # Italy trend
  ITtrend <- dat %>% 
    filter(Code == "ITbol09.03.2020",
           Sex == "b")
  
  # Calculate decomposition
  DecITtrend <- as.data.table(dat)[,
                                   kitagawa_cfr(Cases,ascfr,ITtrend$Cases, ITtrend$ascfr),
                                   by=list(Country, Code, Date, Sex)]
  
  # Select only Italy
  DecITtrend <- DecITtrend %>% filter(Country=="Italy" & Sex=="b") 
  
  # Only keep interesting variables
  DecITtrend <- DecITtrend %>% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp)
  
  # Relative contributions
  DecITtrend <- DecITtrend %>% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecITtrend <- DecITtrend %>% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  # Rename
  DecITtrend <- DecITtrend %>% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp)
  
  # Sort data
  DecITtrend <- DecITtrend %>% arrange(Date)
  
  
### Appendix: Trends USA/Spain ################################################
  
  ### NYC trend
  NYtrend <- dat %>% 
    filter(Code == "US_NYC22.03.2020",
           Sex == "b")
  
  # Calculate decomposition
  DecNYtrend <- as.data.table(dat)[,
                                   kitagawa_cfr(Cases,ascfr,NYtrend$Cases, NYtrend$ascfr),
                                   by=list(Country, Region,Code, Date, Sex)]
  
  # Select only NYC
  DecNYtrend <- DecNYtrend %>% filter(Country=="USA" & Region=="NYC" & Sex=="b") 
  
  # Only keep interesting variables
  DecNYtrend <- DecNYtrend %>% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp)
  
  # Relative contributions
  DecNYtrend <- DecNYtrend %>% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecNYtrend <- DecNYtrend %>% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  # Rename
  DecNYtrend <- DecNYtrend %>% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp)
  
  # Sort data
  DecNYtrend <- DecNYtrend %>% arrange(Date)
  
  ### Spain trend
  EStrend <- dat %>% 
    filter(Code == "ES21.03.2020",
           Sex == "b")
  
  # Calculate decomposition
  DecEStrend <- as.data.table(dat)[,
                                   kitagawa_cfr(Cases,ascfr,EStrend$Cases, EStrend$ascfr),
                                   by=list(Country, Code, Date, Sex)]
  
  # Select only Spain
  DecEStrend <- DecEStrend %>% filter(Country=="Spain" & Sex=="b") 
  
  # Only keep interesting variables
  DecEStrend <- DecEStrend %>% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp)
  
  # Relative contributions
  DecEStrend <- DecEStrend %>% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecEStrend <- DecEStrend %>% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  # Rename
  DecEStrend <- DecEStrend %>% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp)
  
  # Sort data
  DecEStrend <- DecEStrend %>% arrange(Date)
  
  ### Germany trend
  DEtrend <- dat %>% 
    filter(Code == "DE_21.03.2020",
           Sex == "b")
  
  # Calculate decomposition
  DecDEtrend <- as.data.table(dat)[,
                                   kitagawa_cfr(Cases,ascfr,DEtrend$Cases, DEtrend$ascfr),
                                   by=list(Country, Code, Date, Sex)]
  
  # Select only Germany
  DecDEtrend <- DecDEtrend %>% filter(Country=="Germany" & Sex=="b" & Date>="2020-03-21") 
  
  # Only keep interesting variables
  DecDEtrend <- DecDEtrend %>% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp)
  
  # Relative contributions
  DecDEtrend <- DecDEtrend %>% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecDEtrend <- DecDEtrend %>% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))
  
  # Rename
  DecDEtrend <- DecDEtrend %>% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp)
  
  # Sort data
  DecDEtrend <- DecDEtrend %>% arrange(Date)
  
  
### Save results ##############################################################
  
  # Table 2
  write_xlsx(x=DecSK,
             path="files/COVID_article/Table2.xlsx")
  
  # Table 3
  write_xlsx(x=DecITtrend,
             path="files/COVID_article/Table3.xlsx")
  
  # Appendix table 1
  write_xlsx(x=DecDE,
             path="files/COVID_article/AppendixTab1.xlsx")
  
  # Appendix table 2
  write_xlsx(x=DecIT,
             path="files/COVID_article/AppendixTab2.xlsx")
  
  # Appendix table 3
  write_xlsx(x=DecNYtrend,
             path="files/COVID_article/AppendixTab3.xlsx")
  
  # Appendix table 4
  write_xlsx(x=DecEStrend,
             path="files/COVID_article/AppendixTab4.xlsx")
  
  # Appendix table 5
  write_xlsx(x=DecDEtrend,
             path="files/COVID_article/AppendixTab5.xlsx")
```

```{r extra analysis, eval = FALSE}
### Load functions & packages #################################################
 
# Not necessary anymore, all the libraries are stored together above. 
 ## source(("00_functions.R"))
  

### Load case data ############################################################

  # Load data
  cases <- read_csv("files/COVID_article/inputdata.csv")
  
  # Edit date
  cases$Date <- as.Date(cases$Date,"%d.%m.%y")
  
  # Lists of countries and regions
  countrylist <- c("China","Germany","Italy","South Korea","Spain","USA")
  regionlist <- c("All")
  
  # Restrict
  cases <- cases %>% filter(Country %in% countrylist & Region %in% regionlist)
  
  # Drop tests
  cases <- cases %>% mutate(Tests=NULL)
  
  
### Load and edit excess mortality data #######################################
  
  # Load CSV file
  dat <- read_csv("files/COVID_article/baseline_excess_pclm_5.csv")
  
  # Set Date as date
  dat$Date <- as.Date(dat$date,"%d.%m.%y")

  # Restrict
  # Restrict
  dat <- dat %>% filter(Country %in% countrylist) %>% 
    filter(Date >= "2020-02-24")


### Analysis similar to Table 2 ###############################################
  
  # Generate cumulative excess deaths
  dat <- dat %>% 
    mutate(exc_p = ifelse(excess < 0, 0, excess)) %>%
    group_by(Country,Age,Sex) %>% 
    mutate(Exc = cumsum(exc_p)) %>% ungroup()
  
  # Edit age variable
  dat <- dat %>% mutate(Age=recode(Age,
                                   '5'=0,
                                   '15'=10,
                                   '25'=20,
                                   '35'=30,
                                   '45'=40,
                                   '55'=50,
                                   '65'=60,
                                   '75'=70,
                                   '85'=80,
                                   '95'=90))
  
  # Aggregate
  dat <- dat %>% group_by(Country,Sex,Date,Age,Week) %>% 
    select(Exc) %>% summarize_all(sum)
  
  # Adjust date for US: case countrs from two days earlier than excess mortality
  cases$Date[cases$Date=="2020-05-23" & cases$Country=="USA"] <- "2020-05-25"
  
  # Merge with cases
  dat <- inner_join(dat,cases[,c("Country","Date","Age","Sex","Cases")])

  # Calculate ASFRs
  dat <- dat %>% mutate(ascfr = Exc / Cases,
                        ascfr = replace_na(ascfr, 0),
                        ascfr = ifelse(is.infinite(ascfr),0,ascfr),
                        ascfr = ifelse(ascfr>1,1,ascfr))
  
  # Decide some reference patterns (here Germany)
  DE <- dat %>% 
    filter(Country == "Germany",
           Sex == "b",
           #Date == maxdate)
           Week == 19)

  
  # Decompose
  DecDE <- as.data.table(dat)[,
                              kitagawa_cfr(DE$Cases, DE$ascfr,Cases,ascfr),
                              by=list(Country,Week, Sex)]
  
  # Select only most recent date, both genders combined
  DecDE <- DecDE %>% filter(Sex=="b") %>% group_by(Country) %>% filter(Week %in% 19:22)

  # Drop unnecessary variables
  DecDE <- DecDE %>% select(Country,Week,CFR2,Diff,AgeComp,RateComp)

  # Calculate relative contributions
  DecDE <- DecDE %>% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp)))
  DecDE <- DecDE %>% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp)))

  # Rename
  DecDE <- DecDE %>% rename(DiffDE=Diff,AgeCompDE=AgeComp,RateCompDE=RateComp)

  # Sort data
  DecDE <- DecDE %>% arrange(CFR2) # Appendix


### Save extra table ##########################################################
  
  # Appendix table 1
  write_xlsx(x=DecDE,
            path="files/COVID_article/AppendixTab6.xlsx")
  
```

```{r line graph}
 # Load data
  db_gh <- read_csv("files/COVID_article/inputdata.csv")


### Aggregate data ############################################################

  # Filter date
  db_gh$Date <- as.Date(db_gh$Date,"%d.%m.%y")
  db_gh2 <- db_gh %>% filter(Date<=as.Date("30.06.2020","%d.%m.%y"))
  
  # Set New York as "country" (easier handling)
  db_gh2$Country[db_gh2$Country=="USA" & db_gh2$Region == "NYC"] <- "NYC"
  
  # Sum data over age groups
  db_gh2 <- db_gh2 %>% 
    filter(!Country %in% c("China","USA","South Korea") & Sex == "b") %>% 
    group_by(Country, Code,Date) %>% 
    summarise(Cases = sum(Cases),
              Deaths = sum(Deaths))

  # Exclude bolletino 
  db_gh2 <- db_gh2 %>%
    filter(str_sub(Code, 1, 5) != "ITbol")
  
  # Sort by date
  db_gh2 <- db_gh2 %>% group_by(Country) %>% arrange(Date)
  
  # Smooth reporting issues cases
  for(country in unique(db_gh2$Country)) {
    
    days <- db_gh2$Date[db_gh2$Country==country]
    
    for(day in 2:length(days)) {
      current <- db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day]]
      previous <- db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day-1]]
      
      if(current<previous) db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day]] <- previous
      
    }
    
  }

  # Smooth reporting issues deaths
  for(country in unique(db_gh2$Country)) {
    
    days <- db_gh2$Date[db_gh2$Country==country]
    
    for(day in 2:length(days)) {
      current <- db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day]]
      previous <- db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day-1]]
      
      if(current<previous) db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day]] <- previous
      
    }
    
  }
  

### Plot settings #############################################################

  # Set colors
  col_country <- c("Germany" = "black",
                   "Italy" = "#2ca25f",
                   "NYC"="#f0027f",
                   "Spain"="#beaed4",
                   "South Korea"="#fdc086")#,
                   #"USA"="#386cb0")
  
  cols <- c("black",
            "#2ca25f",
            "#f0027f",
            "#beaed4",
            "#fdc086")#,
            #"#386cb0")
  
  
  # Axis
  labs <- db_gh2 %>%
    group_by(Country) %>% 
    filter(Cases == max(Cases)) %>% 
    mutate(Cases = Cases + 3000)
  
  # Including all reports
  tx <- 6
  lim_x <- 240000

### Plot ######################################################################

  db_gh2 %>% 
    ggplot(aes(Cases, Deaths, col = Country))+
    geom_line(size = 1, alpha = .9)+
    scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+
    scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+
    annotate("segment", x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = "grey40", size = .5, alpha = .3, linetype = 2)+
    annotate("segment", x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = "grey40", size = .5, alpha = .3, linetype = 2)+
    annotate("segment", x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = "grey40", size = .5, alpha = .3, linetype = 2)+
    annotate("segment", x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = "grey40", size = .5, alpha = .3, linetype = 2)+
    annotate("text", label = "2% CFR", x = lim_x + 1000, y = lim_x * .02,
             color="grey30", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +
    annotate("text", label = "5% CFR", x = lim_x + 1000, y = lim_x * .05,
             color="grey30", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +
    annotate("text", label = "10% CFR", x = lim_x + 1000, y = lim_x * .10,
             color="grey30", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +
    annotate("text", label = "15% CFR", x = lim_x + 1000, y = lim_x * .15,
             color="grey30", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +
    scale_colour_manual(values = cols)+
    geom_text(data = labs, aes(Cases, Deaths, label = Country),
              size = tx * .35, hjust = 0, fontface = "bold") +
    theme_classic()+
    labs(x = "Cases", 
         y = "Deaths")+
    theme(
      panel.grid.minor = element_blank(),
      legend.position = "none",
      plot.margin = margin(5,5,5,5,"mm"),
      axis.text.x = element_text(size = tx),
      axis.text.y = element_text(size = tx),
      axis.title.x = element_text(size = tx + 1),
      axis.title.y = element_text(size = tx + 1)
    )
```

_Figure 1: Differences in Case fatality rate between countries._

```{r save plot}
  
  # Save
  ggsave("files/COVID_article/Fig_1.jpg", width = 4, height = 3, dpi = 600)
  
```
</details>