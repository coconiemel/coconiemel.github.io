[["index.html", "Curriculum Vitae Contact information Education Professional Experience Skills Languages", " Curriculum Vitae Contact information Utrecht, The Netherlands +31 6 48 48 34 68 cocojn@hotmail.com github.com/coconiemel Education Biology and medical laboratory research, University of applied sciences Utrecht 2019 - today Minor in Data science for biology. Propaedeutic phase completed. Physical therapy, University of applied sciences Utrecht 2017 - 2019 Professional Experience Pracht jewelry store Seller and Fashion advisor Utrecht 2022 - today Sell different types of jewelry. Give advise. Compose wedding or event jewelry sets. Saltro, COVID-19 pandemic laboratory Medical microbiology analist Utrecht 2021 Checking incoming samples. Analyze samples for COVID-19. Return results to GGD. The Sales Unit Teamcaptain Utrecht 2019 - 2021 Recruit donors for charities. Responsibility for a team of 4/5 people. Provide weekly salestraining. Restaurant Het Kabinet Bartender The Hague 2017 - 2019 Albert Heijn Cashier The Hague 2016 - 2017 Pet shop Renee van der Westen Seller The Hague 2015 Skills R, Bash, CSS Adobe Photoshop, Adobe Premiere Pro Languages Dutch, native speaker English, C1 Spanish, B1 "],["reproducible-research.html", "1 Reproducible Research Introduction Analyse an excel file 1.1 Figure 2: Normalised scatterplot of C. elegans offspring counts. Scoring reproducibility Usage of other peoples codes", " 1 Reproducible Research Introduction During this assignment I learned to search, analyse and use different types of reproducible data. Analyse an excel file The experiment used for this analysis was obtained from an experiment in which adult C. elegans was exposed to varying concentrations of different compounds. The data for this experiment is supplied by J. Louter (INT/ILC). The experiment contains a Excel file called CE.LIQ.FLOW.062_Tidydata.xlsx. During analysing this file I noticed that under ‘compVehicle’ the negative and positive seems to be alike, apart from the concentration. Negative is a higher concentration with ethanol and positive is a lower concentration with a variation of things. The summary of the file tells the positive control is ethanol and the negative control is S medium. # Obtain file in environment CE.LIQ.FLOW.062 &lt;- read_excel(&quot;files/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) After import in R I checked the datatypes, but these were not assigned correctly. RawData is numeric, but should be integer, because it contains full numbers. CompName is character, but should be factor, because the names can be changed with numbers and get sorted by. CompConcentration is character, but should be numeric, because it contains decimals. # Check datatype multi_check(CE.LIQ.FLOW.062$RawData, CE.LIQ.FLOW.062$compName, CE.LIQ.FLOW.062$compConcentration) To keep working with the file, I changed the datatypes to the correct ones. # Turn into integer datatype. CE.LIQ.FLOW.062$RawData &lt;- as.integer(CE.LIQ.FLOW.062$RawData) # Turn into factor. CE.LIQ.FLOW.062$compName &lt;- as.factor(CE.LIQ.FLOW.062$compName) # Turn into double (numeric value). CE.LIQ.FLOW.062$compConcentration &lt;- as.double(CE.LIQ.FLOW.062$compConcentration) After this plotting was possible. The scatterplot shows the log10 of the concentration of different compounds in the C. elegans offspring count. The correction with log10 was necessary because the labels of the x-axis were overlapping each other, the values had a large number of decimals and were randomly positioned. # Obtain needed columns CE.LIQ.FLOW.062_summary &lt;- CE.LIQ.FLOW.062 %&gt;% group_by(compName, compConcentration, expType) %&gt;% summarize(mean_counts = mean(RawData, na.rm = TRUE), sd_counts = sd(RawData, na.rm = TRUE),) CE.LIQ.FLOW.062_summary_tbl &lt;- CE.LIQ.FLOW.062_summary %&gt;% filter(expType == &quot;experiment&quot;) # Create scatterplot CE.LIQ.FLOW.062_summary_tbl %&gt;% ggplot(aes(x = log10(compConcentration), y = mean_counts)) + geom_point(aes(color = compName, shape = expType), size = 2, position = position_jitter(width = 0.5, height = 0.5, seed = 123)) + geom_errorbar(aes(ymin=mean_counts-sd_counts, ymax=mean_counts+sd_counts, color = compName), width= 0.2, position = position_jitter(width = 0.5, height = 0.5, seed = 123))+ labs(title = &quot;Mean counts C. elegans offspring in various concentrations&quot;, x = &quot;Log10 of compound concentration&quot;, y = &quot;C. elegans offspring count&quot;) + theme(axis.text = element_text(size = 10)) Figure 1: Scatterplot of C. elegans offspring counts. For further analysing of the effect I would start by testing normality by performing a Shapiro-Wilk test. If unusual, I would normalise the data, if normal, I would start an ANOVA between de different conditions. I would end with the post-hoc tests, to check in what combination of groups the difference is. In this case, the data is not normal, so normalising is necessary. The normalising step is to see if the offspring count increases or decreases relatively to the negative control and if so, if there is anything influenced by the negative control. # Viewing CE.LIQ.FLOW.062_summary shows the mean value of controlNegative = 85.9. # Add the column with normalised average counts CE.LIQ.FLOW.062_summary_norm &lt;- CE.LIQ.FLOW.062_summary_tbl %&gt;% mutate(norm_counts = mean_counts/85.9, norm_sd = sd_counts/85.9) # Create scatterplot with normalised data CE.LIQ.FLOW.062_summary_norm %&gt;% ggplot(aes(x = log10(compConcentration), y = norm_counts)) + geom_point(aes(color = compName, shape = expType), size = 2, position = position_jitter(width = 0.5, height = 0.5, seed = 150)) + geom_errorbar(aes(ymin=norm_counts-norm_sd, ymax=norm_counts+norm_sd, color = compName), width= 0.2, position = position_jitter(width = 0.5, height = 0.5, seed = 150))+ labs(title = &quot;Mean counts C. elegans offspring in various concentrations&quot;, subtitle = &quot;normalised to negative control = 1&quot;, x = &quot;Log10 of compound concentration&quot;, y = &quot;C. elegans offspring count&quot;) + theme(axis.text = element_text(size = 10)) 1.1 Figure 2: Normalised scatterplot of C. elegans offspring counts. Scoring reproducibility The next analysis is based on an article that addresses if meaningless novel words in linguistic contexts can achieve emotional connotations and whether these connotations can affect the quality of word learning and retention (Snefjella, Lana, and Kuperman 2020). There is a lot of language research concerning the human capacity for learning new words, though this research is by far not complete yet. Because of that, the goal of this paper is to add to this research field. To answer these questions, five experiments were set up. Three groups of L1 speakers of the English language had to learn nine novel words in a consistently positive, neutral or negative context. During the learning phase, reading times were recorded. Immediately after and one week after, vocabulary post-test were administered to assess learning and retention. The results showed that for both research questions two out of three groups learned the forms, definitional meanings and emotional connotations and that de positive group learned the best out of the three groups. The analysis is a scoring on the basis of the Repita transparency criteria. This is a method for identifying reproducibility issues in a scientific publication (Sumner et al. 2020). The scoring is shown below, the definitions can be found in the link above. # Create matrix repita_criteria_tab &lt;- matrix(c(&quot;Study purpose&quot;, &quot;V&quot;, &quot;Data availability statement&quot;, &quot;V&quot;, &quot;Data location&quot;, &quot;[Available here](https://osf.io/yghx3/)&quot;, &quot;Study location&quot;, &quot;USA&quot;, &quot;Author review&quot;, &quot;[Bottom of page](https://www.sciencedirect.com/science/article/abs/pii/S0749596X20300851?via%3Dihub)&quot;, &quot;Ethics statement&quot;, &quot;V&quot;, &quot;Funding statement&quot;, &quot;V&quot;, &quot;Code availability&quot;, &quot;X&quot;), ncol=2, byrow=TRUE) # Define names of matrix colnames(repita_criteria_tab) &lt;- c(&#39;Transparency criteria&#39;, &#39;Score&#39;) # Convert matrix to table repita_criteria_tab &lt;- as.table(repita_criteria_tab) # Design and view table kbl(repita_criteria_tab, caption = &quot;Table 1: The Repita criteria with answers concerning the article above.&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) (#tab:repita scoring table)Table 1: The Repita criteria with answers concerning the article above. Transparency criteria Score A Study purpose V B Data availability statement V C Data location Available here D Study location USA E Author review Bottom of page F Ethics statement V G Funding statement V H Code availability X Usage of other peoples codes The last analysis contains a code of an article about the case-fatality rate of COVID-19 (Dudel et al. 2020). Looking at the code, is seems to achieve a visualisation of the differences in case-fatality rate (CFR) over time in between countries, using the age structure of infection and age specific CFRs. This is done by creating tables containing the latest data of diagnosed cases and death counts (concerning COVID-19) for each country. The readability of the code is quite al right. The code in general is readable and understandable because of comments and different chuncks. But the total code is quite long and stored in different files, which makes it a little bit harder to access and run. After trying the code myself the code was a little less accessible than I thought. At first it took a lot of effort, because the file in the url didn’t exist any more and I couldn’t figure out the updated pathway. The updated pathway was not clearly indicated, but once I found it the reproduction was pretty smooth and I only had to make small changes. The full code is stated below, showing comments of changes I made to make it run. ### Case fatality rate ####################################################### # cc = case-age distribution # rr = age-specific case fatality rates cfr &lt;- function(cc,rr){ sum(cc * rr) } ### Kitagawa decomposition #################################################### # c1 = Age distribution population 1 # r1 = Case fatality rates population 1 # c2 = Age distribution population 2 # r2 = Case fatality rates population 2 kitagawa_cfr &lt;- function(c1, r1, c2, r2){ # Calculate age-distribution of cases c1 &lt;- c1 / sum(c1) c2 &lt;- c2 / sum(c2) # Total difference Tot &lt;- cfr(c1, r1) - cfr(c2, r2) # Age component Aa &lt;- sum((c1 - c2) * (r1 + r2) / 2) # Case fatality component Bb &lt;- sum((r1 - r2) * (c1 + c2) / 2) # Output list(Diff = Tot, AgeComp = Aa, RateComp = Bb, CFR1 = weighted.mean(r1,c1), CFR2 = weighted.mean(r2,c2)) } ## This whole chunck doesn&#39;t work anymore because the file in the url has been deleted by the owner. Instead of this file, a file in the given data, named &quot;inputdata.csv&quot;, will be used. Because of this, the chunck will not be executed (eval = false). # Required packages source((&quot;00_functions.R&quot;)) # URL + filename url &lt;- &#39;https://osf.io/wu5ve//?action=download&#39; filename &lt;- &#39;Data/Output_10.csv&#39; # Load data GET(url, write_disk(filename, overwrite = TRUE)) dat &lt;- read_csv(filename,skip=3) ### Edit data (select countries, etc.) ######################################## # Lists of countries and regions countrylist &lt;- c(&quot;China&quot;,&quot;Germany&quot;,&quot;Italy&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;USA&quot;) region &lt;- c(&quot;All&quot;,&quot;NYC&quot;) # Restrict dat &lt;- dat %&gt;% filter(Country %in% countrylist &amp; Region %in% region) # Remove Tests variable dat &lt;- dat %&gt;% mutate(Tests=NULL) # Drop if no cases/Deaths dat &lt;- na.omit(dat) ### Save ###################################################################### write_csv(dat,path=&quot;Data/inputdata.csv&quot;) ### Load functions &amp; packages ################################################# # Not necessary anymore, all the libraries are stored together above. ## source((&quot;00_functions.R&quot;)) ### Load and edit data ######################################################## # Load CSV file dat &lt;- read_csv(&quot;files/COVID_article/inputdata.csv&quot;) # Set Date as date dat$Date &lt;- as.Date(dat$Date,&quot;%d.%m.%y&quot;) # Find max dates maxdates &lt;- dat %&gt;% group_by(Country,Region) %&gt;% summarize(maxdate=max(Date)) # Get least common denominator maxdate &lt;- maxdates %&gt;% filter(Country!=&quot;China&quot;) %&gt;% ungroup() %&gt;% summarize(min(maxdate)) # As vector maxdate &lt;- as.data.frame(maxdate)[1,1] ### Numbers for Table 1 ####################################################### # Latest date: maxdate refdate &lt;- as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%Y&quot;) dat2 &lt;- dat %&gt;% filter(Date&lt;=refdate) #maxdate # Aggregate case and death counts cases &lt;- aggregate(Cases~Code+Date+Country+Region,data=dat2[dat2$Sex==&quot;b&quot;,],sum) deaths &lt;- aggregate(Deaths~Code+Date+Country+Region,data=dat2[dat2$Sex==&quot;b&quot;,],sum) # Most recent counts cases %&gt;% group_by(Country,Region) %&gt;% dplyr::slice(which.max(Date)) deaths %&gt;% group_by(Country,Region) %&gt;% dplyr::slice(which.max(Date)) ### Analysis for Table 2 (and appendix) ####################################### # Calculate ASFRs dat &lt;- dat %&gt;% mutate(ascfr = Deaths / Cases, ascfr = replace_na(ascfr, 0)) # Get codes for reference countries maxdate &lt;- format.Date(maxdate,&quot;%d.%m.%Y&quot;) refdate &lt;- as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%Y&quot;) refdate2 &lt;- format.Date(refdate,&quot;%d.%m.%Y&quot;)#maxdate DE_code &lt;- paste0(&quot;DE_&quot;,refdate2)#paste0(&quot;DE_&quot;,maxdate) IT_code &lt;- paste0(&quot;ITbol&quot;,refdate2)#paste0(&quot;ITinfo&quot;,maxdate) SK_code &lt;- paste0(&quot;KR&quot;,refdate2)#paste0(&quot;SK&quot;,maxdate) # Decide some reference patterns (For main text: SK) DE &lt;- dat %&gt;% filter(Code == DE_code, Sex == &quot;b&quot;) IT &lt;- dat %&gt;% filter(Code == IT_code, Sex == &quot;b&quot;) SK &lt;- dat %&gt;% filter(Code == SK_code, Sex == &quot;b&quot;) # Decompose DecDE &lt;- as.data.table(dat)[, kitagawa_cfr(DE$Cases, DE$ascfr,Cases,ascfr), by=list(Country, Code, Date, Sex, Region)] DecIT &lt;- as.data.table(dat)[, kitagawa_cfr(IT$Cases, IT$ascfr,Cases,ascfr), by=list(Country, Code, Date, Sex,Region)] DecSK &lt;- as.data.table(dat)[, kitagawa_cfr(SK$Cases, SK$ascfr,Cases,ascfr), by=list(Country, Code, Date, Sex,Region)] # Select only most recent date, both genders combined DecDE &lt;- DecDE %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country,Region) %&gt;% filter(Date&lt;=refdate) %&gt;% dplyr::slice(which.max(Date)) DecIT &lt;- DecIT %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country,Region) %&gt;% filter(Date&lt;=refdate) %&gt;% dplyr::slice(which.max(Date)) DecSK &lt;- DecSK %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country,Region) %&gt;% filter(Date&lt;=refdate) %&gt;% dplyr::slice(which.max(Date)) # Drop unnecessary variables DecDE &lt;- DecDE %&gt;% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp) DecIT &lt;- DecIT %&gt;% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp) DecSK &lt;- DecSK %&gt;% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp) # Calculate relative contributions DecDE &lt;- DecDE %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecDE &lt;- DecDE %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) DecIT &lt;- DecIT %&gt;% mutate(relAgeIT = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecIT &lt;- DecIT %&gt;% mutate(relRateIT = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) DecSK &lt;- DecSK %&gt;% mutate(relAgeSK = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecSK &lt;- DecSK %&gt;% mutate(relRateSK = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecDE &lt;- DecDE %&gt;% rename(DiffDE=Diff,AgeCompDE=AgeComp,RateCompDE=RateComp) DecIT &lt;- DecIT %&gt;% rename(DiffIT=Diff,AgeCompIT=AgeComp,RateCompIT=RateComp) DecSK &lt;- DecSK %&gt;% rename(DiffSK=Diff,AgeCompSK=AgeComp,RateCompSK=RateComp) # Sort data DecDE &lt;- DecDE %&gt;% arrange(CFR2) # Appendix DecIT &lt;- DecIT %&gt;% arrange(CFR2) # Appendix DecSK &lt;- DecSK %&gt;% arrange(CFR2) # Table 2 ### Table 3: Italy trend ###################################################### # Italy trend ITtrend &lt;- dat %&gt;% filter(Code == &quot;ITbol09.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecITtrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,ITtrend$Cases, ITtrend$ascfr), by=list(Country, Code, Date, Sex)] # Select only Italy DecITtrend &lt;- DecITtrend %&gt;% filter(Country==&quot;Italy&quot; &amp; Sex==&quot;b&quot;) # Only keep interesting variables DecITtrend &lt;- DecITtrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecITtrend &lt;- DecITtrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecITtrend &lt;- DecITtrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecITtrend &lt;- DecITtrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecITtrend &lt;- DecITtrend %&gt;% arrange(Date) ### Appendix: Trends USA/Spain ################################################ ### NYC trend NYtrend &lt;- dat %&gt;% filter(Code == &quot;US_NYC22.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecNYtrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,NYtrend$Cases, NYtrend$ascfr), by=list(Country, Region,Code, Date, Sex)] # Select only NYC DecNYtrend &lt;- DecNYtrend %&gt;% filter(Country==&quot;USA&quot; &amp; Region==&quot;NYC&quot; &amp; Sex==&quot;b&quot;) # Only keep interesting variables DecNYtrend &lt;- DecNYtrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecNYtrend &lt;- DecNYtrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecNYtrend &lt;- DecNYtrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecNYtrend &lt;- DecNYtrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecNYtrend &lt;- DecNYtrend %&gt;% arrange(Date) ### Spain trend EStrend &lt;- dat %&gt;% filter(Code == &quot;ES21.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecEStrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,EStrend$Cases, EStrend$ascfr), by=list(Country, Code, Date, Sex)] # Select only Spain DecEStrend &lt;- DecEStrend %&gt;% filter(Country==&quot;Spain&quot; &amp; Sex==&quot;b&quot;) # Only keep interesting variables DecEStrend &lt;- DecEStrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecEStrend &lt;- DecEStrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecEStrend &lt;- DecEStrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecEStrend &lt;- DecEStrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecEStrend &lt;- DecEStrend %&gt;% arrange(Date) ### Germany trend DEtrend &lt;- dat %&gt;% filter(Code == &quot;DE_21.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecDEtrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,DEtrend$Cases, DEtrend$ascfr), by=list(Country, Code, Date, Sex)] # Select only Germany DecDEtrend &lt;- DecDEtrend %&gt;% filter(Country==&quot;Germany&quot; &amp; Sex==&quot;b&quot; &amp; Date&gt;=&quot;2020-03-21&quot;) # Only keep interesting variables DecDEtrend &lt;- DecDEtrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecDEtrend &lt;- DecDEtrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecDEtrend &lt;- DecDEtrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecDEtrend &lt;- DecDEtrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecDEtrend &lt;- DecDEtrend %&gt;% arrange(Date) ### Save results ############################################################## # Table 2 write_xlsx(x=DecSK, path=&quot;files/COVID_article/Table2.xlsx&quot;) # Table 3 write_xlsx(x=DecITtrend, path=&quot;files/COVID_article/Table3.xlsx&quot;) # Appendix table 1 write_xlsx(x=DecDE, path=&quot;files/COVID_article/AppendixTab1.xlsx&quot;) # Appendix table 2 write_xlsx(x=DecIT, path=&quot;files/COVID_article/AppendixTab2.xlsx&quot;) # Appendix table 3 write_xlsx(x=DecNYtrend, path=&quot;files/COVID_article/AppendixTab3.xlsx&quot;) # Appendix table 4 write_xlsx(x=DecEStrend, path=&quot;files/COVID_article/AppendixTab4.xlsx&quot;) # Appendix table 5 write_xlsx(x=DecDEtrend, path=&quot;files/COVID_article/AppendixTab5.xlsx&quot;) ### Load functions &amp; packages ################################################# # Not necessary anymore, all the libraries are stored together above. ## source((&quot;00_functions.R&quot;)) ### Load case data ############################################################ # Load data cases &lt;- read_csv(&quot;files/COVID_article/inputdata.csv&quot;) # Edit date cases$Date &lt;- as.Date(cases$Date,&quot;%d.%m.%y&quot;) # Lists of countries and regions countrylist &lt;- c(&quot;China&quot;,&quot;Germany&quot;,&quot;Italy&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;USA&quot;) regionlist &lt;- c(&quot;All&quot;) # Restrict cases &lt;- cases %&gt;% filter(Country %in% countrylist &amp; Region %in% regionlist) # Drop tests cases &lt;- cases %&gt;% mutate(Tests=NULL) ### Load and edit excess mortality data ####################################### # Load CSV file dat &lt;- read_csv(&quot;files/COVID_article/baseline_excess_pclm_5.csv&quot;) # Set Date as date dat$Date &lt;- as.Date(dat$date,&quot;%d.%m.%y&quot;) # Restrict # Restrict dat &lt;- dat %&gt;% filter(Country %in% countrylist) %&gt;% filter(Date &gt;= &quot;2020-02-24&quot;) ### Analysis similar to Table 2 ############################################### # Generate cumulative excess deaths dat &lt;- dat %&gt;% mutate(exc_p = ifelse(excess &lt; 0, 0, excess)) %&gt;% group_by(Country,Age,Sex) %&gt;% mutate(Exc = cumsum(exc_p)) %&gt;% ungroup() # Edit age variable dat &lt;- dat %&gt;% mutate(Age=recode(Age, &#39;5&#39;=0, &#39;15&#39;=10, &#39;25&#39;=20, &#39;35&#39;=30, &#39;45&#39;=40, &#39;55&#39;=50, &#39;65&#39;=60, &#39;75&#39;=70, &#39;85&#39;=80, &#39;95&#39;=90)) # Aggregate dat &lt;- dat %&gt;% group_by(Country,Sex,Date,Age,Week) %&gt;% select(Exc) %&gt;% summarize_all(sum) # Adjust date for US: case countrs from two days earlier than excess mortality cases$Date[cases$Date==&quot;2020-05-23&quot; &amp; cases$Country==&quot;USA&quot;] &lt;- &quot;2020-05-25&quot; # Merge with cases dat &lt;- inner_join(dat,cases[,c(&quot;Country&quot;,&quot;Date&quot;,&quot;Age&quot;,&quot;Sex&quot;,&quot;Cases&quot;)]) # Calculate ASFRs dat &lt;- dat %&gt;% mutate(ascfr = Exc / Cases, ascfr = replace_na(ascfr, 0), ascfr = ifelse(is.infinite(ascfr),0,ascfr), ascfr = ifelse(ascfr&gt;1,1,ascfr)) # Decide some reference patterns (here Germany) DE &lt;- dat %&gt;% filter(Country == &quot;Germany&quot;, Sex == &quot;b&quot;, #Date == maxdate) Week == 19) # Decompose DecDE &lt;- as.data.table(dat)[, kitagawa_cfr(DE$Cases, DE$ascfr,Cases,ascfr), by=list(Country,Week, Sex)] # Select only most recent date, both genders combined DecDE &lt;- DecDE %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country) %&gt;% filter(Week %in% 19:22) # Drop unnecessary variables DecDE &lt;- DecDE %&gt;% select(Country,Week,CFR2,Diff,AgeComp,RateComp) # Calculate relative contributions DecDE &lt;- DecDE %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecDE &lt;- DecDE %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecDE &lt;- DecDE %&gt;% rename(DiffDE=Diff,AgeCompDE=AgeComp,RateCompDE=RateComp) # Sort data DecDE &lt;- DecDE %&gt;% arrange(CFR2) # Appendix ### Save extra table ########################################################## # Appendix table 1 write_xlsx(x=DecDE, path=&quot;files/COVID_article/AppendixTab6.xlsx&quot;) # Load data db_gh &lt;- read_csv(&quot;files/COVID_article/inputdata.csv&quot;) ### Aggregate data ############################################################ # Filter date db_gh$Date &lt;- as.Date(db_gh$Date,&quot;%d.%m.%y&quot;) db_gh2 &lt;- db_gh %&gt;% filter(Date&lt;=as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%y&quot;)) # Set New York as &quot;country&quot; (easier handling) db_gh2$Country[db_gh2$Country==&quot;USA&quot; &amp; db_gh2$Region == &quot;NYC&quot;] &lt;- &quot;NYC&quot; # Sum data over age groups db_gh2 &lt;- db_gh2 %&gt;% filter(!Country %in% c(&quot;China&quot;,&quot;USA&quot;,&quot;South Korea&quot;) &amp; Sex == &quot;b&quot;) %&gt;% group_by(Country, Code,Date) %&gt;% summarise(Cases = sum(Cases), Deaths = sum(Deaths)) # Exclude bolletino db_gh2 &lt;- db_gh2 %&gt;% filter(str_sub(Code, 1, 5) != &quot;ITbol&quot;) # Sort by date db_gh2 &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% arrange(Date) # Smooth reporting issues cases for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } # Smooth reporting issues deaths for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } ### Plot settings ############################################################# # Set colors col_country &lt;- c(&quot;Germany&quot; = &quot;black&quot;, &quot;Italy&quot; = &quot;#2ca25f&quot;, &quot;NYC&quot;=&quot;#f0027f&quot;, &quot;Spain&quot;=&quot;#beaed4&quot;, &quot;South Korea&quot;=&quot;#fdc086&quot;)#, #&quot;USA&quot;=&quot;#386cb0&quot;) cols &lt;- c(&quot;black&quot;, &quot;#2ca25f&quot;, &quot;#f0027f&quot;, &quot;#beaed4&quot;, &quot;#fdc086&quot;)#, #&quot;#386cb0&quot;) # Axis labs &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% filter(Cases == max(Cases)) %&gt;% mutate(Cases = Cases + 3000) # Including all reports tx &lt;- 6 lim_x &lt;- 240000 ### Plot ###################################################################### db_gh2 %&gt;% ggplot(aes(Cases, Deaths, col = Country))+ geom_line(size = 1, alpha = .9)+ scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+ scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;text&quot;, label = &quot;2% CFR&quot;, x = lim_x + 1000, y = lim_x * .02, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;5% CFR&quot;, x = lim_x + 1000, y = lim_x * .05, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;10% CFR&quot;, x = lim_x + 1000, y = lim_x * .10, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;15% CFR&quot;, x = lim_x + 1000, y = lim_x * .15, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + scale_colour_manual(values = cols)+ geom_text(data = labs, aes(Cases, Deaths, label = Country), size = tx * .35, hjust = 0, fontface = &quot;bold&quot;) + theme_classic()+ labs(x = &quot;Cases&quot;, y = &quot;Deaths&quot;)+ theme( panel.grid.minor = element_blank(), legend.position = &quot;none&quot;, plot.margin = margin(5,5,5,5,&quot;mm&quot;), axis.text.x = element_text(size = tx), axis.text.y = element_text(size = tx), axis.title.x = element_text(size = tx + 1), axis.title.y = element_text(size = tx + 1) ) Figure 3: Differences in Case fatality rate between countries. # Save ggsave(&quot;files/COVID_article/Fig_1.jpg&quot;, width = 4, height = 3, dpi = 600) "],["data-management.html", "2 Data management Introduction My execution", " 2 Data management Introduction I learned to use the Guerrilla analytics framework to manage my data in a clear way. The Guerrilla analytics framework consists of seven principles, which I will shortly introduce (Gestel, n.d.). Space is cheap, confusion is expensive. Saying that storage costs are low. Always keep your files, store them in a reliable online cloud and protect yourself from cyber criminals. Use simple, visual project structures and conventions. Organise files and folders in a system that makes it understandable for other team members. A couple of things to think of is to avoid deep nesting in folders, create a separate folder for each project and use sub folders for different datasets. Also try not to change names or move them, this makes it harder to loose. Automate with program code. This makes reproducibility possible. Link stored data to data in the analytics environment to data in work products. When storing data on different platforms, make sure to link those platforms before anything gets lost. Version control changes to data and analytics code. I used Gihub for version control, to secure all changes and data. Consolidate team knowledge. When working together, make sure to have a good communication line. Use code that runs from start to finish. This also makes it easier to reproduce or revisit later on. My execution An example of applying the Guerrilla analytics framework is linked below. This shows a project of a previous course I followed (DAUR2). Figure 4: Folder tree following Guerrilla analytics framework. "],["a-phylogenetic-analysis.html", "3 A phylogenetic analysis Introduction Plan The Filoviridae family The phylogenetic tree Future steps", " 3 A phylogenetic analysis Introduction For this assignment, we were challenged to learn a new skill in four days, any skill related to data science and life sciences that will help you in the future. After this course I will be specialising in microbiology, this is also where I see myself in a couple of years. Data science for me is a great opportunity to combine different work areas and keep my work various, but my main focus will be microbiology. Before I start working in the field, I hope to earn a masters degree, so in two years I will probably be busy with my masters. I am not sure what masters I would like to do. Because I am interested in microbiology, my free assignment will also be in that area. I chose to learn to generate a phylogenetic tree and to analyse its contents about the Filoviridae family. I chose this viral family because I am interested in infectious diseases and two species of these are known to cause quite the infectious diseases. Plan To achieve my goal, I had 4 days total planned to work on it. I had divided the days in a couple groups: Half a day to come up with a plan, write this and search the right data needed. One day to find everything that I need for my own analysis. Two days to implement my knowledge from searching the internet and tutorials to the data I will be using and complete generating the tree. Half a day to finish the writing and details. Eventually, I gained information that I needed to generate a phylogenetic tree from a lot of searching, some trial and error, but also with help from these resources: Multiple Alignment Object ggtree utilities tree Visualization Building a phylogeny in R Visualization and annotation of phylogenetic trees: ggtree The Filoviridae family Viruses of the Filoviridae family, also know as filoviruses, are viruses containing one molecule of single-stranded negative-sense RNA that are enveloped in a fatty membrane. The viruses are zoonotic and it is believed that their reservoir hosts usually are bats (“Filoviruses (Filoviridae) | Viral Hemorrhagic Fevers (VHFs) | CDC” 2021). The viruses are one of the families known to cause Viral Haemorrhagic Fevers (VHFs) that affects multiple body systems and usually results in heavy bleeding and death (“What Are VHFs? | Viral Hemorrhagic Fevers (VHFs) | CDC” 2021). There are currently five genera in the family; Ebolavirus, Marburgvirus, Cuevavirus, Dianlovirus and Striavirus. Ebolavirus has six known species and the rest have one (Languon and Quaye 2019). In my analysis I left Striavirus out, because there is barely any information or data on. Ebolavirus and Marburgvirus are the most common to cause (deathly) disease in humans, the other ones also host in non-human primates and pigs (Bucko and Gieger, n.d.). The phylogenetic tree In one of my resources I found a useful function from the {compbio4all} package, but Nathan L Brouwer, the writer of the package, states that if there is any trouble downloading the package the (following) code for the function can be copied. The package in question can be found at the github repository. I did have trouble installing the package, so the code retrieved from github is below. # Function to clean up fasta files fasta_cleaner &lt;- function(fasta_object, parse = TRUE){ fasta_object &lt;- sub(&quot;^(&gt;)(.*?)(\\\\n)(.*)(\\\\n\\\\n)&quot;,&quot;\\\\4&quot;,fasta_object) fasta_object &lt;- gsub(&quot;\\n&quot;, &quot;&quot;, fasta_object) if(parse == TRUE){ fasta_object &lt;- stringr::str_split(fasta_object, pattern = &quot;&quot;, simplify = FALSE) } return(fasta_object[[1]]) } # Function to store fasta sequences into a R list entrez_fetch_list &lt;- function(db, id, rettype, ...){ #setup list for storing output n.seq &lt;- length(id) list.output &lt;- as.list(rep(NA, n.seq)) names(list.output) &lt;- id # get output for(i in 1:length(id)){ list.output[[i]] &lt;- rentrez::entrez_fetch(db = db, id = id[i], rettype = rettype) } return(list.output) } The analysis starts by fetching the sequences from NCBI nucleotide bank (“National Center for Biotechnology Information,” n.d.) and make them compatible for analysis. The file “species” is written to enhance the target IDs, this way, all the genomes can be fetched at once. # Import list of target species species &lt;-scan(file = (&quot;files/filo_data/species.txt&quot;), what = &quot;character&quot;, sep = &quot;\\n&quot;, comment.char = &quot;#&quot;) # Obtain fasta sequences of all species combined in a list filoviridae &lt;- entrez_fetch_list(db = &quot;nucleotide&quot;, id = species, rettype = &quot;fasta&quot;) # Remove non sequence infromation for(i in 1:length(filoviridae)){ filoviridae[[i]] &lt;- fasta_cleaner(filoviridae[[i]], parse = F) } # Create string set filoviridae_vector &lt;- rep(NA, length(filoviridae)) for(i in 1:length(filoviridae_vector)){ filoviridae_vector[i] &lt;- filoviridae[[i]] } names(filoviridae_vector) &lt;- names(filoviridae) filoviridae_vector_ss &lt;- Biostrings::AAStringSet(filoviridae_vector) Next is the multiple sequence alignment. This creates an alignment to easily spot the distances between all the options. Because this takes a long time to perform, I saved the alignment in a file that can be obtained. # Perform multiple sequence alignment filoviridae_align &lt;- msa(filoviridae_vector_ss, method = &quot;ClustalW&quot;) # Change class into correct one, in this case RNA because filoviruses are RNA viruses class(filoviridae_align) &lt;- &quot;RNAMultipleAlignment&quot; # Save in file so alignment only needs to run once saveRDS(filoviridae_align, (&quot;files/filo_data/filoviridae_align.RDS&quot;)) # Import aligned file for usage filoviridae_align &lt;- readRDS(&quot;files/filo_data/filoviridae_align.RDS&quot;) To use the alignment for the tree, the genetic distance between the sequences will be calculated. For readability, a rounded version is also made. After the calculations the tree is plotted. The tree clearly shows that most ebolaviruses are closely related, which is logical because they are only different species, only Reston and Sudan are somewhat further from the rest but close to each other. It also shows that Mengla dianlovirus and Marburg marburgvirus are closely related, and Lloviu is also nearby. # Adjust datatype for calculations filoviridae_align_seqinr &lt;- msaConvert(filoviridae_align, type = &quot;seqinr::alignment&quot;) # Calculate genetic distances filoviridae_dist &lt;- seqinr::dist.alignment(filoviridae_align_seqinr, matrix = &quot;identity&quot;) filoviridae_dist_rounded &lt;- round(filoviridae_dist, digits = 2) # Add rounded version # Generate the structure of the tree filoviridae_tree &lt;- nj(filoviridae_dist) # Plot tree ggtree(filoviridae_tree, ladderize = T, branch.length = &quot;branch.length&quot;, root.position = .05)+ geom_tiplab(size = 3.5)+ xlim(0, .8)+ labs(title = &quot;Filoviridae family tree&quot;, subtitle = &quot;Rooted phylogenetic tree, with branch lengths&quot;)+ theme(plot.title = element_text(size = 18, face = &quot;bold&quot;, hjust = .5), plot.subtitle = element_text(size = 12, face = &quot;italic&quot;, hjust = .5)) Figure 5: Phylogenetic tree of the Filoviridae family. The tree shows an easy picture of the differences, but is doesn’t show the numbers in detail. To solve this, I calculated them separately. The response of the code shows the calculated genetic distances of all options. # Details filoviridae_dist_rounded ## KC545393.1 Bundibogyo ebolavirus ## MH121167.1 Taiforest ebolavirus 0.56 ## KT357858.1 Zaire ebolavirus 0.61 ## NC_039345.1 Bombali ebolavirus 0.64 ## EU338380.1 Sudan ebolavirus 0.65 ## MF540571.1 Reston ebolavirus 0.65 ## MZ541881.1 Lloviu cuevavirus 0.70 ## OL702894.1 Marburg marburgvirus 0.75 ## NC_055510.1 Mengla dianlovirus 0.75 ## MH121167.1 Taiforest ebolavirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus 0.61 ## NC_039345.1 Bombali ebolavirus 0.64 ## EU338380.1 Sudan ebolavirus 0.65 ## MF540571.1 Reston ebolavirus 0.65 ## MZ541881.1 Lloviu cuevavirus 0.70 ## OL702894.1 Marburg marburgvirus 0.76 ## NC_055510.1 Mengla dianlovirus 0.75 ## KT357858.1 Zaire ebolavirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus ## NC_039345.1 Bombali ebolavirus 0.63 ## EU338380.1 Sudan ebolavirus 0.65 ## MF540571.1 Reston ebolavirus 0.65 ## MZ541881.1 Lloviu cuevavirus 0.71 ## OL702894.1 Marburg marburgvirus 0.75 ## NC_055510.1 Mengla dianlovirus 0.75 ## NC_039345.1 Bombali ebolavirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus ## NC_039345.1 Bombali ebolavirus ## EU338380.1 Sudan ebolavirus 0.66 ## MF540571.1 Reston ebolavirus 0.66 ## MZ541881.1 Lloviu cuevavirus 0.70 ## OL702894.1 Marburg marburgvirus 0.76 ## NC_055510.1 Mengla dianlovirus 0.76 ## EU338380.1 Sudan ebolavirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus ## NC_039345.1 Bombali ebolavirus ## EU338380.1 Sudan ebolavirus ## MF540571.1 Reston ebolavirus 0.63 ## MZ541881.1 Lloviu cuevavirus 0.71 ## OL702894.1 Marburg marburgvirus 0.76 ## NC_055510.1 Mengla dianlovirus 0.76 ## MF540571.1 Reston ebolavirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus ## NC_039345.1 Bombali ebolavirus ## EU338380.1 Sudan ebolavirus ## MF540571.1 Reston ebolavirus ## MZ541881.1 Lloviu cuevavirus 0.71 ## OL702894.1 Marburg marburgvirus 0.76 ## NC_055510.1 Mengla dianlovirus 0.75 ## MZ541881.1 Lloviu cuevavirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus ## NC_039345.1 Bombali ebolavirus ## EU338380.1 Sudan ebolavirus ## MF540571.1 Reston ebolavirus ## MZ541881.1 Lloviu cuevavirus ## OL702894.1 Marburg marburgvirus 0.76 ## NC_055510.1 Mengla dianlovirus 0.76 ## OL702894.1 Marburg marburgvirus ## MH121167.1 Taiforest ebolavirus ## KT357858.1 Zaire ebolavirus ## NC_039345.1 Bombali ebolavirus ## EU338380.1 Sudan ebolavirus ## MF540571.1 Reston ebolavirus ## MZ541881.1 Lloviu cuevavirus ## OL702894.1 Marburg marburgvirus ## NC_055510.1 Mengla dianlovirus 0.67 mima(filoviridae_dist_rounded) # 0.56, 0.76 ## $min ## [1] 0.56 ## ## $max ## [1] 0.76 Seen from here it states that Taiforest ebolavirus &amp; Bundibogyo ebolavirus are closest related with a score of 0.56. It also states that there are multiple options that score furthest related with a score of 0.76. These are: Bombali ebolavirus &amp; Marburg marburgvirus, Bombali ebolavirus &amp; Mengla dianlovirus, Sudan ebolavirus &amp; Marburg marburgvirus, Sudan ebolavirus &amp; Mengla dianlovirus, Reston ebolavirus &amp; Marburg marburgvirus, Lloviu cuevavirus &amp; Marburg marburgvirus, Lloviu cuevavirus &amp; Mengla dianlovirus. Future steps In the future I would like to expand my knowledge in phylogeny and generating trees, because this was just a start, there are so much more options to create. A couple of examples of things I would still like to learn in this area are: How to add the genetic distances to the tree so it is clear in one view. How to generate different types of trees, because there are a lot more and for different data, different types of trees are more compatible. How to read an extensive tree. The one I’ve created is doable, but once they get a lot more values, reading it becomes quite hard. "],["group-project.html", "4 Group project Introduction ONTOX Our addition", " 4 Group project Introduction Part of the course is a group project I peformed with two others. Our project is about ONTOX and their implementation of the Phymdos app. ONTOX The ONTOX Consortium is a research initiative dedicated to the study and application of ontological technology to develop solutions for real-world problems. The consortium is made up of leading experts in ontology and semantic technology from around the world. The research team is focused on developing ontology-based applications to address problems in the areas of health, environment, and security. Using ontology-based frameworks, the consortium is developing innovative solutions for data integration and data sharing, as well as for managing and exploiting huge amounts of data and actively exploring the use of ontology-based methods for decision support and artificial intelligence applications. The European Commission is investing in this concept and has recently launched the “ONTOX” project, which is focused on providing a functional and sustainable solution for assessing the human risk of chemicals without animal testing. The project will create new approach methodologies (NAMs) which use computational systems based on artificial intelligence and are fed by biological, toxicological, chemical and kinetic data. These NAMs will be able to predict systemic repeated dose toxicity effects and, when combined with tailored exposure assessment, will allow for human risk assessment. ONTOX is expected to have a long-lasting effect, reinforcing Europe’s role in the development and application of animal-free methods for risk assessment of chemicals (Vinken et al. 2021). As they state themselves: “The vision of the ONTOX consortium is to provide a functional and sustainable solution for advancing human risk assessment of chemicals without the use of animals in line with the principles of 21st century toxicity testing and next generation risk assessment.” (“ONTOX Project,” n.d.) One of the ways to achieve this is via the Phymdos app, created by D. Roodzant for University of Applied Sciences Utrecht. The app collects and combines existing data for risk assessment using the SysRev platform, this platform helps with data curation, SERs (Systematic Evidence Reviews), and managed reviews. It has been used to create thousands of projects, covering a wide range of disciplines whom are all publicly accessible (Bozada et al. 2021). In this case, the platform extracts metabolic pathways and compound interactions from data. The data is then formatted into an SBtab file which can be used to create visualisations or other formats such as SBML. SBtab is a data format designed for Systems Biology that builds from the structure of spreadsheets. It defines conventions for table structure, controlled vocabularies, and semantic annotations to support automated data integration and model building. There are predefined table types for experimental data and SBML-compliant models, and the format can be easily customised to include new types of data (Lubitz et al. 2016). Our addition Our goal is to add to the function of the Phymdos app. The app now only allows to extract and analyse from one single tab, but to apply this process to a larger scope, it should be combining data from multiple sources instead of just focusing on one. So, the assignment we have received from Marc Teunis (The ONTOX project) is to retrieve the SBtab data from multiple articles and to merge these multiple files into one new file. Additionally, we must create a graph to visualise the data and must convert de SBtab file to a SBML format. This will all be combined in a R package (“Project ONTOX: Data Sciences for Biology 2,” n.d.). "],["relational-data-and-databases.html", "5 Relational data and databases Introduction DBeaver connection Inspection Descriptive statistics Visualisations", " 5 Relational data and databases Introduction This assignment was focused on learning the basics of the SQL language and how to work this around relational data and databases. These are collections of information stored in separate tables with underlying relations to create an easy overview of relations between different data structures. DBeaver connection I start by creating three data frames out of files and making them tidy. # The flu ## Import, first 11 lines are metadata flu_df &lt;- read_csv(&quot;files/flu_data.csv&quot;, skip = 11) ## Make tidy flu_df_tidy &lt;- flu_df %&gt;% pivot_longer(cols = c(2:ncol(flu_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Dengue ## Import, first 11 lines are metadata dengue_df &lt;- read_csv(&quot;files/dengue_data.csv&quot;, skip = 11) ## Make tidy dengue_df_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = c(2:ncol(dengue_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Gapminder ## The dataframe is part of the dslabs package and therefore does not need to be imported. ## The dataframe is already tidy. An important part of relational data is enabling comparison across different data frames. In this case we will be looking at the country and date, but in the data frames we use there are a couple of things that needs to be adjusted to make them similar. # Column &#39;country&#39; ## Check types. multi_check(flu_df_tidy$country, dengue_df_tidy$country, gapminder$country) # character, character, factor ## Change all types into factor flu_df_tidy$country &lt;- as.factor(flu_df_tidy$country) dengue_df_tidy$country &lt;- as.factor(dengue_df_tidy$country) # Column &#39;date&#39; ## Check types. multi_check(flu_df_tidy$Date, dengue_df_tidy$Date, gapminder$year) ## Because flu and dengue are specified to day instead of the year at gapminder, a &#39;year&#39; column needs to be added and changed into the integer type to create similarity. flu_df_tidy &lt;- flu_df_tidy %&gt;% mutate(year = substr(flu_df_tidy$Date, start=1, stop=4)) dengue_df_tidy &lt;- dengue_df_tidy %&gt;% mutate(year = substr(dengue_df_tidy$Date, start=1, stop=4)) ## Change all types into integer flu_df_tidy$year &lt;- as.integer(flu_df_tidy$year) dengue_df_tidy$year &lt;- as.integer(dengue_df_tidy$year) The data frames are now similar and can be stored into csv and rds files. # Export data frames as csv and rds save_to_csv_rds(flu_df_tidy, &quot;files/flu&quot;) save_to_csv_rds(dengue_df_tidy, &quot;files/dengue&quot;) save_to_csv_rds(gapminder, &quot;files/gapminder&quot;) DBeaver and R were connected (my real password is hidden) and the tables were inserted into the ‘workflowsdb’ database in DBeaver for inspection. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;Password&quot;) # Hide my real password dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbWriteTable(con, &quot;flu&quot;, flu_df_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_df_tidy) Inspection Now that the tables are imported, I checked a couple of things. Dengue has no “NULL” variables in country and year, 6263 cases in the data set and the average amount of cases is 0.14 with a standard deviation of 0.14. The country with the least amount of cases is Argentina with 0 cases and the country with the most is Venezuela with 1 case. The flu has no “NULL variables in country and year, 17266 cases in the data set and the average amount of cases is 473.74 with a standard deviation of 768.95. The country with the least amount of cases is Argentina with 0 cases and the country with the most is Uruguay 10555 cases. Gapminder had no “NULL” variables in country and year. There was an important thing that stood out to me about the metadata. I will discuss this later in this chapter. This information is obtained by the following SQL and R code: Figure 6: SQL code of data inspection. # Dengue mima(dengue_df_tidy$cases) # 0, 1 ## $min ## [1] 0 ## ## $max ## [1] 1 mean_sd(dengue_df_tidy$cases) # 0.14, 0.14 ## $mean ## [1] 0.1389711 ## ## $sd ## [1] 0.1389235 # Flu mima(flu_df_tidy$cases) # 0, 10555 ## $min ## [1] 0 ## ## $max ## [1] 10555 mean_sd(flu_df_tidy$cases) # 473.74, 768.95 ## $mean ## [1] 473.7355 ## ## $sd ## [1] 768.9549 For a good analysis of the different, but related data, they need to be joined together in one table with the important columns. I created four joined tables, flu with dengue, dengue with gapminder, flu with gapminder and all three of them. Some tables have more countries than others, this is because the joining only joins the matching countries. See below for the SQL and R code. Figure 7: SQL code of inner-join. # Load joined table to object gapminder_flu &lt;- read_csv(&quot;files/gapminder_flu.csv&quot;) gapminder_dengue &lt;- read_csv(&quot;files/gapminder_dengue.csv&quot;) flu_dengue &lt;- read_csv(&quot;files/flu_dengue.csv&quot;) gapminder_flu_dengue &lt;- read_csv(&quot;files/gapminder_flu_dengue.csv&quot;) As said before, there was something important that stood out to me, the dataset missed metadata. The cases columns didn’t specify how it was measured, and this was also not written down in the metadata. This resulted in unreliable data that doesn’t take into account that countries have different populations, just a count of cases isn’t enough. Therefore a normalisation was necessary. I divided the cases by the population from the gapminder data and multiplied this by 100000, which seemed like a logical amount when calculating with populations. I added the results to the tables. gapminder_flu &lt;- gapminder_flu %&gt;% mutate(&quot;cases_per_100000&quot; =gapminder_flu$flu_cases/gapminder_flu$population*100000) gapminder_dengue &lt;- gapminder_dengue %&gt;% mutate(&quot;cases_per_100000&quot; =gapminder_dengue$dengue_cases/gapminder_dengue$population*100000) Descriptive statistics Now that the sets are normalised, descriptives can start. Starting by calculating the means and standard deviations of cases per country with the updated data. After performing the Shapiro-Wilk test, the flu seems to have 8 non normal distributed variables from a total of 29, and dengue has 5 non normal distributed variables from a total of 10. As an extra test, the Levene’s test shows that the flu doesn’t have a variance of equility between countries. Because of these two specifics, a Kruskal-Wallis test was chosen, because this test doesn’t account for normality. The results of this test shows that both datasets have a p-value of &lt; 2.2e-16 which tells us that there is a significance difference. To determine the differences between variables, the Dunn test was performed as a post hoc test. The results of those are stored in separate tables of difference and no difference, but in total 114 from a total of 406 combinations of the flu show differences and 19 from a total of 45 combinations of dengue show differences. # Obtain mean and standard deviation per country flu_sum &lt;- gapminder_flu %&gt;% group_by(country)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) dengue_sum &lt;- gapminder_dengue %&gt;% group_by(country)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) # To check the normality, dataframes are made with the countries flu_country &lt;- gapminder_flu %&gt;% select(country, cases_per_100000, year) %&gt;% pivot_wider(names_from = country, values_from = cases_per_100000) %&gt;% filter(year != 2002) dengue_country &lt;- gapminder_dengue %&gt;% select(country, cases_per_100000, year) %&gt;% pivot_wider(names_from = country, values_from = cases_per_100000) %&gt;% filter(year != 2002) # Shapiro wilk ## Flu flu_stats &lt;- data.frame(country = colnames(flu_country[,2:length(colnames(flu_country))]), pvalue_shap = round(c(shapiro.test(flu_country$Uruguay)$p.value, shapiro.test(flu_country$`United States`)$p.value, shapiro.test(flu_country$Ukraine)$p.value, shapiro.test(flu_country$Switzerland)$p.value, shapiro.test(flu_country$Sweden)$p.value, shapiro.test(flu_country$Spain)$p.value, shapiro.test(flu_country$`South Africa`)$p.value, shapiro.test(flu_country$Russia)$p.value, shapiro.test(flu_country$Romania)$p.value, shapiro.test(flu_country$Poland)$p.value, shapiro.test(flu_country$Peru)$p.value, shapiro.test(flu_country$Paraguay)$p.value, shapiro.test(flu_country$Norway)$p.value, shapiro.test(flu_country$`New Zealand`)$p.value, shapiro.test(flu_country$Netherlands)$p.value, shapiro.test(flu_country$Mexico)$p.value, shapiro.test(flu_country$Japan)$p.value, shapiro.test(flu_country$Hungary)$p.value, shapiro.test(flu_country$Germany)$p.value, shapiro.test(flu_country$France)$p.value, shapiro.test(flu_country$Chile)$p.value, shapiro.test(flu_country$Canada)$p.value, shapiro.test(flu_country$Bulgaria)$p.value, shapiro.test(flu_country$Brazil)$p.value, shapiro.test(flu_country$Bolivia)$p.value, shapiro.test(flu_country$Belgium)$p.value, shapiro.test(flu_country$Austria)$p.value, shapiro.test(flu_country$Australia)$p.value, shapiro.test(flu_country$Argentina)$p.value), 4)) ## Dengue dengue_stats &lt;- data.frame(country = colnames(dengue_country[,2:length(colnames(dengue_country))]), pvalue_shap = round(c(shapiro.test(dengue_country$Venezuela)$p.value, shapiro.test(dengue_country$Thailand)$p.value, shapiro.test(dengue_country$Singapore)$p.value, shapiro.test(dengue_country$Philippines)$p.value, shapiro.test(dengue_country$Mexico)$p.value, shapiro.test(dengue_country$Indonesia)$p.value, shapiro.test(dengue_country$India)$p.value, shapiro.test(dengue_country$Brazil)$p.value, shapiro.test(dengue_country$Bolivia)$p.value, shapiro.test(dengue_country$Argentina)$p.value), 4)) # Normality check flu_stats &lt;- flu_stats %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) # 3 out of 5 are normally distributed dengue_stats &lt;- dengue_stats %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) # 21 out of 29 are normally distributed # Levene test leveneTest(cases_per_100000 ~ country, data = gapminder_flu) # &lt;2.2e-16 ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 28 12.482 &lt; 2.2e-16 *** ## 330 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 leveneTest(cases_per_100000 ~ country, data = gapminder_dengue) # 3.228e-07 ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 9 6.2485 3.288e-07 *** ## 120 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Kruskal Wallis test kruskal.test(cases_per_100000 ~ country, data = gapminder_flu) # p-value &lt; 2.2e-16 ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by country ## Kruskal-Wallis chi-squared = 332.13, df = 28, p-value &lt; 2.2e-16 kruskal.test(cases_per_100000 ~ country, data = gapminder_dengue) # p-value &lt; 2.2e-16 ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by country ## Kruskal-Wallis chi-squared = 110.54, df = 9, p-value &lt; 2.2e-16 # P-value of basically .0000, there is a statistically significant difference # Dunn post-hoc test dengue_dunn &lt;- dunnTest(cases_per_100000 ~ country, data = gapminder_dengue, method = &quot;holm&quot;)$res flu_dunn &lt;- dunnTest(cases_per_100000 ~ country, data = gapminder_flu, method = &quot;holm&quot;)$res # Check for significant differences dengue_dunn &lt;- dengue_dunn %&gt;% mutate(different = P.adj&lt;0.05) flu_dunn &lt;- flu_dunn %&gt;% mutate(different = P.adj&lt;0.05) # Note the significant differences between countries/continents dengue_dunn_diff &lt;- dengue_dunn %&gt;% filter(different == TRUE) flu_dunn_diff &lt;- flu_dunn %&gt;% filter(different == TRUE) # Note the comparable countries/continents dengue_dunn_non &lt;- dengue_dunn %&gt;% filter(different == FALSE) flu_dunn_non &lt;- flu_dunn %&gt;% filter(different == FALSE) Visualisations According to the dataset en information in this assignment, two graphs per disease were made to showcase the data. Flu gapminder_flu %&gt;% ggplot(aes(x=continent,y=cases_per_100000, group=continent, fill=continent)) + geom_col(stat=&quot;identity&quot;)+ labs(title = &quot;Flu cases per continent&quot;, x=&quot;Continent&quot;, y=&quot;Flu cases per 100000 residents&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) Figure 8: Bargraph of the amount of flu cases per continent. ggplot(flu_dengue, aes(x=year, y=flu_cases, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Flu cases&quot;, x = &quot;Year&quot;, y = &quot;Average dengue cases per year&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) Figure 9: Linegraph of average flu cases per year per country. Dengue gapminder_dengue %&gt;% ggplot(aes(x=continent,y=cases_per_100000, group=continent, fill=continent)) + geom_col(stat=&quot;identity&quot;)+ labs(title = &quot;Dengue cases per continent&quot;, x=&quot;Continent&quot;, y=&quot;Dengue cases per 100000 residents&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) Figure 10: Bargraph of the amount of dengue cases per continent ggplot(flu_dengue, aes(x=year, y=dengue_cases, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Dengue cases&quot;, x = &quot;Year&quot;, y = &quot;Average dengue cases per year&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) Figure 11: Linegraph of average dengue cases per year per country. "],["r-package.html", "6 R package Introduction Copopa Install and load", " 6 R package Introduction I created a R package to support my portfolio, following The Whole Game demo made by Hadley Wickham. The demo guided me through setting up my own package and helped me create something useful on my own. Copopa To start a new package, a name is necessary. I combined my name, “portfolio” and “package” together into a fun word and checked availability with the {available} package. The name was available, so I set up the project and the github repository to get started. A package to support my portfolio would reduce duplicated code. But scanning through my portfolio, I noticed that there wasn’t a lot of code or duplicated code. Luckily, I found some that would work for this. The package that I made contains four functions, which I will elaborate following the next example dataset: # Create data frame name &lt;- c(&quot;Noa&quot;, &quot;Luke&quot;, &quot;Oliver&quot;, &quot;Anna&quot;, &quot;Julie&quot;) gender &lt;- c(&quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;) age &lt;- c(21, 29, 27, 29, 24) test &lt;- data.frame(name, gender, age) kbl(test, caption = &quot;Table 2: Example data frame for copopa package&quot;) %&gt;% kable_paper(bootstrap_options = &quot;striped&quot;, full_width = F) (#tab:example data)Table 2: Example data frame for copopa package name gender age Noa F 21 Luke M 29 Oliver M 27 Anna F 29 Julie F 24 multi_check() This functions checks the data types of up to three datasets, or columns in it. Full sets can be used as input, or in this case columns. multi_check(test$name, test$gender, test$age) ## [[1]] ## [1] &quot;character&quot; ## ## [[2]] ## [1] &quot;character&quot; ## ## [[3]] ## [1] &quot;numeric&quot; mima() This function calculates the minimum and maximum of data. The data must be numeric. mima(test$age) ## $min ## [1] 21 ## ## $max ## [1] 29 mean_sd() This function calculates the mean and standard deviation of data. The data must be numeric. mean_sd(test$age) ## $mean ## [1] 26 ## ## $sd ## [1] 3.464102 save_to_csv_rds() This function saves a data frame to a CSV file and a RSD file. Fill in target data set and the name of the wanted output. save_to_csv_rds(test, &quot;test&quot;) For the manual and contents of the package, please visit the copopa github repository. Install and load # Only if not obtained already install.packages(&quot;devtools&quot;) library(devtools) # Installation devtools::install_github(&quot;coconiemel/copopa&quot;) # Load library(copopa) "],["rmarkdown-parameters.html", "7 RMarkdown parameters Introduction Data inspection Building a parametrised graph 7.1 Figure 12: Parametrised graph containing the COVID deaths data in default settings. Alter parameter settings", " 7 RMarkdown parameters Introduction Parameters makes a report in RMarkdown more dynamic, which helps reproduce the analysis on different inputs or sets. Parameters can be set up for a number of things, so changing anything will become easy. In this assignment I will be using data about COVID19 retrieved from ECDC and I will be looking at the daily reported cases and the daily reported deaths. For this assignment I used three parameters, the default of these need to be set in the YAML header, so if the parameters aren’t specified, these will be rendered. --- params: country: &quot;Netherlands&quot; year: 2021 month: 3 --- Data inspection Starting the analysis with inspecting the dataset, without parameters, to look what we’re working with. # Add file as data using the link from the site covid &lt;- read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/nationalcasedeath_eueea_daily_ei/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # Check column names and amount of columns colnames(covid) # There are 11 columns # Inspect data covid %&gt;% count(countriesAndTerritories) # 30 countries participate n_distinct(covid$geoId) # Each country has an unique geoId, so there are no double countries unique(covid$continentExp) # All countries are located in Europe mima(covid$dateRep) # First data was noted on 01/01/2020, last data was noted on 31/12/2021. Building a parametrised graph I will be making two graphs, one about the daily reported cases and one about the daily reported deaths. The parameters are country, year, and months and the default settings are used. The default settings show the data of the Netherlands in all 12 months of the year 2021. # Filter the necessary columns and change the datatype of dateRep. covid_cases &lt;- covid %&gt;% filter(countriesAndTerritories == params$country, year == params$year, month == params$month) covid_cases$dateRep &lt;- as.Date(covid_cases$dateRep, &quot;%d/%m/%y&quot;) # Plot the graph ggplot(covid_cases, aes(x=dateRep, y=cases))+ geom_line()+ geom_point()+ labs(title = &quot;Daily reported covid cases in the Netherlands that occured in March 2021&quot;, x = &quot;Months&quot;, y = &quot;New cases&quot;)+ theme_minimal() Figure 11: Parametrised graph containing the COVID cases data in default settings. # Filter the necessary columns and change the datatype of dateRep. covid_deaths &lt;- covid %&gt;% filter(countriesAndTerritories == params$country, year == params$year, month == params$month) covid_deaths$dateRep &lt;- as.Date(covid_cases$dateRep, &quot;%d/%m/%y&quot;) # Plot the graph ggplot(covid_deaths, aes(x=dateRep, y=deaths))+ geom_line()+ geom_point()+ labs(title = &quot;Daily reported covid deaths in the Netherlands that occured in March 2021&quot;, x = &quot;Months&quot;, y = &quot;Covid related deaths&quot;)+ theme_minimal() 7.1 Figure 12: Parametrised graph containing the COVID deaths data in default settings. Alter parameter settings A parametrised report makes it easy to change settings to have a different view of the data, I added screenshots below to showcase what the graph would look like with a different input. Figure 13: Parametrised graph containing the COVID cases data. Figure 14: Parametrised graph containing the COVID deaths data. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
