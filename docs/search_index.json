[["index.html", "Portfolio Curriculum Vitae Contact information Education Professional Experience Skills Languages", " Portfolio Coco Niemel 2023-01-04 Curriculum Vitae Contact information Utrecht, The Netherlands cocojn@hotmail.com +31 6 48 48 34 68 github.com/coconiemel Education Biology and medical labratory research, University of applied sciences Utrecht 2019 - today Minor in Data science for biology. Propaedeutic phase completed. Physical therapy, University of applied sciences Utrecht 2017 - 2019 Professional Experience Pracht jewelry store Seller and Fashion advisor Utrecht 2022 - today Sell different types of jewelry. Give advise. Compose wedding or event jewelry sets. Saltro, COVID-19 pandemic laboratory Medical microbiology analist Utrecht 2021 Checking incoming samples. Analyze samples for COVID-19. Return results to GGD. The Sales Unit Teamcaptain Utrecht 2019 - 2021 Recruit donors for charities. Responsibility for a team of 4/5 people. Provide weekly salestraining. Restaurant Het Kabinet Bartender The Hague 2017 - 2019 Albert Heijn Cashier The Hague 2016 - 2017 Pet shop Renee van der Westen Seller The Hague 2015 Skills R, Bash Adobe Photoshop, Adobe Premiere Pro Languages Dutch, native speaker English, C1 Spanish, B1 "],["reproducible-research-1.html", "Reproducible Research (1) Introduction The analyzation of a file Scoring reproducibility Usage of other peoples codes", " Reproducible Research (1) Introduction During this assignment I learned to search and analyze different types of reproducible data. The analyzation of a file The experiment used for this analysis was obtained from an experiment in which adult C. elegans was exposed to varying concentrations of different compounds. The data for this experiment is supplied by J. Louter (INT/ILC). # Used libraries in this exercise. library(readxl) library(tidyverse) library(kableExtra) The experiment contains a Excel file called CE.LIQ.FLOW.062_Tidydata.xlsx. During analyzation of this file I noticed that under ‘compVehicle’ the negative and positive seems to be alike, apart from the concentration. Negative is a higher concentration with ethanol and positive is a lower concentration with a variation of things. The summary of the file tells the positive control is ethanol and the negative control is S medium. CE.LIQ.FLOW.062 &lt;- read_excel(&quot;files/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) # redownload file into server When importing and opening the file in R I checked the datatypes, as shown below these were not assigned correctly. RawData should be integer, because it contains full numbers. CompName should be factor, because the names can be changed with numbers and get sorted by. CompConcentration should be double, because it contains decimals. # Check data types. typeof(CE.LIQ.FLOW.062$RawData) # double ## [1] &quot;double&quot; typeof(CE.LIQ.FLOW.062$compName) # character ## [1] &quot;character&quot; typeof(CE.LIQ.FLOW.062$compConcentration) # character ## [1] &quot;character&quot; To keep working with the file, I changed the datatypes to the correct ones. After this plotting was possible. The scatterplot shows the log10 of the concentration of different compounds in the C. elegans offspring count. The correction with log10 was necessary because the labels of the x-axis were overlapping each other, the values had a large number of decimals and were randomly positioned. # Turn raw data into integer datatype. CE.LIQ.FLOW.062$RawData &lt;- as.integer(CE.LIQ.FLOW.062$RawData) # Turn comp name into factor. CE.LIQ.FLOW.062$compName &lt;- as.factor(CE.LIQ.FLOW.062$compName) # Turn comp concentration into double (numeric value). CE.LIQ.FLOW.062$compConcentration &lt;- as.double(CE.LIQ.FLOW.062$compConcentration) # Check data types again. class(CE.LIQ.FLOW.062$RawData) # integer ## [1] &quot;integer&quot; class(CE.LIQ.FLOW.062$compName) # factor ## [1] &quot;factor&quot; class(CE.LIQ.FLOW.062$compConcentration) # numeric ## [1] &quot;numeric&quot; # Correct! # Obtain needed columns. CE.LIQ.FLOW.062_summary &lt;- CE.LIQ.FLOW.062 %&gt;% group_by(compName, compConcentration, expType) %&gt;% summarize(mean_counts = mean(RawData, na.rm = TRUE), sd_counts = sd(RawData, na.rm = TRUE),) CE.LIQ.FLOW.062_summary_tbl &lt;- CE.LIQ.FLOW.062_summary %&gt;% filter(expType == &quot;experiment&quot;) # Create scatterplot. CE.LIQ.FLOW.062_summary_tbl %&gt;% ggplot(aes(x = log10(compConcentration), y = mean_counts)) + geom_point(aes(color = compName, shape = expType), size = 2, position = position_jitter(width = 0.5, height = 0.5, seed = 123)) + geom_errorbar(aes(ymin=mean_counts-sd_counts, ymax=mean_counts+sd_counts, color = compName), width= 0.2, position = position_jitter(width = 0.5, height = 0.5, seed = 123))+ labs(title = &quot;Mean counts C. elegans offspring in various concentrations&quot;, x = &quot;Log10 of compound concentration&quot;, y = &quot;C. elegans offspring count&quot;) + theme(axis.text = element_text(size = 10)) For further analysation of the effect i would start by testing normality by performing a Shapiro-Wilk test. If unusual, i would normalize the data, if normal, i would start an ANOVA between de different conditions. I would end with the post-hoc tests, to check in what combination of groups the difference is. In this case, the data is not normal, so normalization is necessary. The normalization step is to see is the offspring count increases or decreases relatively to the negative control and so, if there is anything influenced by the negative control. # Viewing CE.LIQ.FLOW.062_summary shows the mean value of controlNegative = 85.9. # Add the column with normalized average counts CE.LIQ.FLOW.062_summary_norm &lt;- CE.LIQ.FLOW.062_summary_tbl %&gt;% mutate(norm_counts = mean_counts/85.9, norm_sd = sd_counts/85.9) # Create scatterplot with normalized data. CE.LIQ.FLOW.062_summary_norm %&gt;% ggplot(aes(x = log10(compConcentration), y = norm_counts)) + geom_point(aes(color = compName, shape = expType), size = 2, position = position_jitter(width = 0.5, height = 0.5, seed = 150)) + geom_errorbar(aes(ymin=norm_counts-norm_sd, ymax=norm_counts+norm_sd, color = compName), width= 0.2, position = position_jitter(width = 0.5, height = 0.5, seed = 150))+ labs(title = &quot;Mean counts C. elegans offspring in various concentrations&quot;, subtitle = &quot;Normalized to negative control = 1&quot;, x = &quot;Log10 of compound concentration&quot;, y = &quot;C. elegans offspring count&quot;) + theme(axis.text = element_text(size = 10)) Scoring reproducibility The next analysis is based on an article that addresses if meaningless novel words in linguistic contexts can achieve emotional connotations and whether these connotations can affect the quality of word learning and retention (Snefjella, Lana, and Kuperman 2020). There is a lot of language research concerning the human capacity for learning new words, though this research is by far not complete yet. Because of that, the goal of this paper is to add to this research field. To answer these questions, five experiments were set up. three groups of L1 speakers of the English language had to learn nine novel words in a consistently positive, neutral or negative context. During the learning phase, reading times were recorded. Immediately after and one week after, vocabulary post-test were administered to assess learning and retention. The results showed that for both research questions two out of three groups learned the forms, definitional meanings en emotional connotations and that de positive group learned the best out of the three groups. Part of this analyzation is a scoring on the basis of the “Repita” criteria. This is a method for identifying reproductibility issues in a scientific publication (sumnerReproducibilityReportingPractices2020?). The scoring is shown below. # Create matrix repita_criteria_tab &lt;- matrix(c(&quot;Study purpose&quot;, &quot;V&quot;, &quot;Data availability statement&quot;, &quot;V&quot;, &quot;Data location&quot;, &quot;Online version&quot;, &quot;Study location&quot;, &quot;USA&quot;, &quot;Author review&quot;, &quot;V&quot;, &quot;Ethics statement&quot;, &quot;V&quot;, &quot;Funding statement&quot;, &quot;V&quot;, &quot;Code availability&quot;, &quot;X&quot;), ncol=2, byrow=TRUE) # Define names of matrix colnames(repita_criteria_tab) &lt;- c(&#39;Transparency criteria&#39;, &#39;Score&#39;) # Convert matrix to table repita_criteria_tab &lt;- as.table(repita_criteria_tab) # Design and view table kable(repita_criteria_tab) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Transparency criteria Score A Study purpose V B Data availability statement V C Data location Online version D Study location USA E Author review V F Ethics statement V G Funding statement V H Code availability X Usage of other peoples codes The last analyzation contains a code of an article about the case-fatality rate of COVID-19 (Dudel et al. 2020). Looking at the code, is seems to achieve a visualisation of the differences in case-fatality rate (CFR) over time in between countries, using the age structure of infection and age specific CFRs. This is done by creating tables containing the latest data of diagnosed cases and death counts (concerning COVID-19) for each country. The readability of the code is quite alright. The code in general is readable and understandable because of comments en different chuncks. But the total code is quite long and stored in different files, which makes it a little bit harder to access and run. But after trying the code myself the code was a little less accesible as i thought. At first it took a lot of effort, because the file in the url didn’t exist anymore and I couldn’t figure out the updated pathway. The updated pathway was not clearly indicated, but once I found it the reproduction was pretty smooth and I only had to make small changes. The code is stated below, showing comments of parts i have changed to make the code run smoothly. ### Load packages ############################################################# ## I put al the libraries together here, instead of using &#39;source&#39; per chunck for a better overview. library(tidyverse) library(data.table) library(writexl) library(httr) library(ggrepel) library(scales) ### Case fatality rate ####################################################### # cc = case-age distribution # rr = age-specific case fatality rates cfr &lt;- function(cc,rr){ sum(cc * rr) } ### Kitagawa decomposition #################################################### # c1 = Age distribution population 1 # r1 = Case fatality rates population 1 # c2 = Age distribution population 2 # r2 = Case fatality rates population 2 kitagawa_cfr &lt;- function(c1, r1, c2, r2){ # Calculate age-distribution of cases c1 &lt;- c1 / sum(c1) c2 &lt;- c2 / sum(c2) # Total difference Tot &lt;- cfr(c1, r1) - cfr(c2, r2) # Age component Aa &lt;- sum((c1 - c2) * (r1 + r2) / 2) # Case fatality component Bb &lt;- sum((r1 - r2) * (c1 + c2) / 2) # Output list(Diff = Tot, AgeComp = Aa, RateComp = Bb, CFR1 = weighted.mean(r1,c1), CFR2 = weighted.mean(r2,c2)) } ## This whole chunck doesn&#39;t work anymore because the file in the url has been deleted by the owner. Instead of this file, a file in the given data, named &quot;inputdata.csv&quot;, will be used. Because of this, the chunck will not be executed (eval = false). # Required packages source((&quot;00_functions.R&quot;)) # URL + filename url &lt;- &#39;https://osf.io/wu5ve//?action=download&#39; filename &lt;- &#39;Data/Output_10.csv&#39; # Load data GET(url, write_disk(filename, overwrite = TRUE)) dat &lt;- read_csv(filename,skip=3) ### Edit data (select countries, etc.) ######################################## # Lists of countries and regions countrylist &lt;- c(&quot;China&quot;,&quot;Germany&quot;,&quot;Italy&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;USA&quot;) region &lt;- c(&quot;All&quot;,&quot;NYC&quot;) # Restrict dat &lt;- dat %&gt;% filter(Country %in% countrylist &amp; Region %in% region) # Remove Tests variable dat &lt;- dat %&gt;% mutate(Tests=NULL) # Drop if no cases/Deaths dat &lt;- na.omit(dat) ### Save ###################################################################### write_csv(dat,path=&quot;Data/inputdata.csv&quot;) ### Load functions &amp; packages ################################################# # Not necessary anymore, all the libraries are stored together above. ## source((&quot;00_functions.R&quot;)) ### Load and edit data ######################################################## # Load CSV file dat &lt;- read_csv(&quot;files/COVID_article/inputdata.csv&quot;) # Set Date as date dat$Date &lt;- as.Date(dat$Date,&quot;%d.%m.%y&quot;) # Find max dates maxdates &lt;- dat %&gt;% group_by(Country,Region) %&gt;% summarize(maxdate=max(Date)) # Get least common denominator maxdate &lt;- maxdates %&gt;% filter(Country!=&quot;China&quot;) %&gt;% ungroup() %&gt;% summarize(min(maxdate)) # As vector maxdate &lt;- as.data.frame(maxdate)[1,1] ### Numbers for Table 1 ####################################################### # Latest date: maxdate refdate &lt;- as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%Y&quot;) dat2 &lt;- dat %&gt;% filter(Date&lt;=refdate) #maxdate # Aggregate case and death counts cases &lt;- aggregate(Cases~Code+Date+Country+Region,data=dat2[dat2$Sex==&quot;b&quot;,],sum) deaths &lt;- aggregate(Deaths~Code+Date+Country+Region,data=dat2[dat2$Sex==&quot;b&quot;,],sum) # Most recent counts cases %&gt;% group_by(Country,Region) %&gt;% slice(which.max(Date)) ## # A tibble: 7 × 5 ## # Groups: Country, Region [7] ## Code Date Country Region Cases ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 CN11.02.2020 2020-02-11 China All 44672 ## 2 DE_30.06.2020 2020-06-30 Germany All 194877. ## 3 ITbol30.06.2020 2020-06-30 Italy All 240455 ## 4 KR30.06.2020 2020-06-30 South Korea All 12800 ## 5 ES21.05.2020 2020-05-21 Spain All 234824. ## 6 US27.06.2020 2020-06-27 USA All 2504175. ## 7 US_NYC30.06.2020 2020-06-30 USA NYC 212072. deaths %&gt;% group_by(Country,Region) %&gt;% slice(which.max(Date)) ## # A tibble: 7 × 5 ## # Groups: Country, Region [7] ## Code Date Country Region Deaths ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 CN11.02.2020 2020-02-11 China All 1023. ## 2 DE_30.06.2020 2020-06-30 Germany All 9078 ## 3 ITbol30.06.2020 2020-06-30 Italy All 33736 ## 4 KR30.06.2020 2020-06-30 South Korea All 282 ## 5 ES21.05.2020 2020-05-21 Spain All 28628. ## 6 US27.06.2020 2020-06-27 USA All 124541 ## 7 US_NYC30.06.2020 2020-06-30 USA NYC 18492 ### Analysis for Table 2 (and appendix) ####################################### # Calculate ASFRs dat &lt;- dat %&gt;% mutate(ascfr = Deaths / Cases, ascfr = replace_na(ascfr, 0)) # Get codes for reference countries maxdate &lt;- format.Date(maxdate,&quot;%d.%m.%Y&quot;) refdate &lt;- as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%Y&quot;) refdate2 &lt;- format.Date(refdate,&quot;%d.%m.%Y&quot;)#maxdate DE_code &lt;- paste0(&quot;DE_&quot;,refdate2)#paste0(&quot;DE_&quot;,maxdate) IT_code &lt;- paste0(&quot;ITbol&quot;,refdate2)#paste0(&quot;ITinfo&quot;,maxdate) SK_code &lt;- paste0(&quot;KR&quot;,refdate2)#paste0(&quot;SK&quot;,maxdate) # Decide some reference patterns (For main text: SK) DE &lt;- dat %&gt;% filter(Code == DE_code, Sex == &quot;b&quot;) IT &lt;- dat %&gt;% filter(Code == IT_code, Sex == &quot;b&quot;) SK &lt;- dat %&gt;% filter(Code == SK_code, Sex == &quot;b&quot;) # Decompose DecDE &lt;- as.data.table(dat)[, kitagawa_cfr(DE$Cases, DE$ascfr,Cases,ascfr), by=list(Country, Code, Date, Sex, Region)] DecIT &lt;- as.data.table(dat)[, kitagawa_cfr(IT$Cases, IT$ascfr,Cases,ascfr), by=list(Country, Code, Date, Sex,Region)] DecSK &lt;- as.data.table(dat)[, kitagawa_cfr(SK$Cases, SK$ascfr,Cases,ascfr), by=list(Country, Code, Date, Sex,Region)] # Select only most recent date, both genders combined DecDE &lt;- DecDE %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country,Region) %&gt;% filter(Date&lt;=refdate) %&gt;% slice(which.max(Date)) DecIT &lt;- DecIT %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country,Region) %&gt;% filter(Date&lt;=refdate) %&gt;% slice(which.max(Date)) DecSK &lt;- DecSK %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country,Region) %&gt;% filter(Date&lt;=refdate) %&gt;% slice(which.max(Date)) # Drop unnecessary variables DecDE &lt;- DecDE %&gt;% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp) DecIT &lt;- DecIT %&gt;% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp) DecSK &lt;- DecSK %&gt;% select(Country,Region,Date,CFR2,Diff,AgeComp,RateComp) # Calculate relative contributions DecDE &lt;- DecDE %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecDE &lt;- DecDE %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) DecIT &lt;- DecIT %&gt;% mutate(relAgeIT = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecIT &lt;- DecIT %&gt;% mutate(relRateIT = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) DecSK &lt;- DecSK %&gt;% mutate(relAgeSK = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecSK &lt;- DecSK %&gt;% mutate(relRateSK = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecDE &lt;- DecDE %&gt;% rename(DiffDE=Diff,AgeCompDE=AgeComp,RateCompDE=RateComp) DecIT &lt;- DecIT %&gt;% rename(DiffIT=Diff,AgeCompIT=AgeComp,RateCompIT=RateComp) DecSK &lt;- DecSK %&gt;% rename(DiffSK=Diff,AgeCompSK=AgeComp,RateCompSK=RateComp) # Sort data DecDE &lt;- DecDE %&gt;% arrange(CFR2) # Appendix DecIT &lt;- DecIT %&gt;% arrange(CFR2) # Appendix DecSK &lt;- DecSK %&gt;% arrange(CFR2) # Table 2 ### Table 3: Italy trend ###################################################### # Italy trend ITtrend &lt;- dat %&gt;% filter(Code == &quot;ITbol09.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecITtrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,ITtrend$Cases, ITtrend$ascfr), by=list(Country, Code, Date, Sex)] # Select only Italy DecITtrend &lt;- DecITtrend %&gt;% filter(Country==&quot;Italy&quot; &amp; Sex==&quot;b&quot;) # Only keep interesting variables DecITtrend &lt;- DecITtrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecITtrend &lt;- DecITtrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecITtrend &lt;- DecITtrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecITtrend &lt;- DecITtrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecITtrend &lt;- DecITtrend %&gt;% arrange(Date) ### Appendix: Trends USA/Spain ################################################ ### NYC trend NYtrend &lt;- dat %&gt;% filter(Code == &quot;US_NYC22.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecNYtrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,NYtrend$Cases, NYtrend$ascfr), by=list(Country, Region,Code, Date, Sex)] # Select only NYC DecNYtrend &lt;- DecNYtrend %&gt;% filter(Country==&quot;USA&quot; &amp; Region==&quot;NYC&quot; &amp; Sex==&quot;b&quot;) # Only keep interesting variables DecNYtrend &lt;- DecNYtrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecNYtrend &lt;- DecNYtrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecNYtrend &lt;- DecNYtrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecNYtrend &lt;- DecNYtrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecNYtrend &lt;- DecNYtrend %&gt;% arrange(Date) ### Spain trend EStrend &lt;- dat %&gt;% filter(Code == &quot;ES21.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecEStrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,EStrend$Cases, EStrend$ascfr), by=list(Country, Code, Date, Sex)] # Select only Spain DecEStrend &lt;- DecEStrend %&gt;% filter(Country==&quot;Spain&quot; &amp; Sex==&quot;b&quot;) # Only keep interesting variables DecEStrend &lt;- DecEStrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecEStrend &lt;- DecEStrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecEStrend &lt;- DecEStrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecEStrend &lt;- DecEStrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecEStrend &lt;- DecEStrend %&gt;% arrange(Date) ### Germany trend DEtrend &lt;- dat %&gt;% filter(Code == &quot;DE_21.03.2020&quot;, Sex == &quot;b&quot;) # Calculate decomposition DecDEtrend &lt;- as.data.table(dat)[, kitagawa_cfr(Cases,ascfr,DEtrend$Cases, DEtrend$ascfr), by=list(Country, Code, Date, Sex)] # Select only Germany DecDEtrend &lt;- DecDEtrend %&gt;% filter(Country==&quot;Germany&quot; &amp; Sex==&quot;b&quot; &amp; Date&gt;=&quot;2020-03-21&quot;) # Only keep interesting variables DecDEtrend &lt;- DecDEtrend %&gt;% select(Country,Code,Date,CFR1,Diff,AgeComp,RateComp) # Relative contributions DecDEtrend &lt;- DecDEtrend %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecDEtrend &lt;- DecDEtrend %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecDEtrend &lt;- DecDEtrend %&gt;% rename(DiffITt=Diff,AgeCompITt=AgeComp,RateCompITt=RateComp) # Sort data DecDEtrend &lt;- DecDEtrend %&gt;% arrange(Date) ### Save results ############################################################## # Table 2 write_xlsx(x=DecSK, path=&quot;files/COVID_article/Table2.xlsx&quot;) # Table 3 write_xlsx(x=DecITtrend, path=&quot;files/COVID_article/Table3.xlsx&quot;) # Appendix table 1 write_xlsx(x=DecDE, path=&quot;files/COVID_article/AppendixTab1.xlsx&quot;) # Appendix table 2 write_xlsx(x=DecIT, path=&quot;files/COVID_article/AppendixTab2.xlsx&quot;) # Appendix table 3 write_xlsx(x=DecNYtrend, path=&quot;files/COVID_article/AppendixTab3.xlsx&quot;) # Appendix table 4 write_xlsx(x=DecEStrend, path=&quot;files/COVID_article/AppendixTab4.xlsx&quot;) # Appendix table 5 write_xlsx(x=DecDEtrend, path=&quot;files/COVID_article/AppendixTab5.xlsx&quot;) ### Load functions &amp; packages ################################################# # Not necessary anymore, all the libraries are stored together above. ## source((&quot;00_functions.R&quot;)) ### Load case data ############################################################ # Load data cases &lt;- read_csv(&quot;files/COVID_article/inputdata.csv&quot;) # Edit date cases$Date &lt;- as.Date(cases$Date,&quot;%d.%m.%y&quot;) # Lists of countries and regions countrylist &lt;- c(&quot;China&quot;,&quot;Germany&quot;,&quot;Italy&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;USA&quot;) regionlist &lt;- c(&quot;All&quot;) # Restrict cases &lt;- cases %&gt;% filter(Country %in% countrylist &amp; Region %in% regionlist) # Drop tests cases &lt;- cases %&gt;% mutate(Tests=NULL) ### Load and edit excess mortality data ####################################### # Load CSV file dat &lt;- read_csv(&quot;files/COVID_article/baseline_excess_pclm_5.csv&quot;) # Set Date as date dat$Date &lt;- as.Date(dat$date,&quot;%d.%m.%y&quot;) # Restrict # Restrict dat &lt;- dat %&gt;% filter(Country %in% countrylist) %&gt;% filter(Date &gt;= &quot;2020-02-24&quot;) ### Analysis similar to Table 2 ############################################### # Generate cumulative excess deaths dat &lt;- dat %&gt;% mutate(exc_p = ifelse(excess &lt; 0, 0, excess)) %&gt;% group_by(Country,Age,Sex) %&gt;% mutate(Exc = cumsum(exc_p)) %&gt;% ungroup() # Edit age variable dat &lt;- dat %&gt;% mutate(Age=recode(Age, &#39;5&#39;=0, &#39;15&#39;=10, &#39;25&#39;=20, &#39;35&#39;=30, &#39;45&#39;=40, &#39;55&#39;=50, &#39;65&#39;=60, &#39;75&#39;=70, &#39;85&#39;=80, &#39;95&#39;=90)) # Aggregate dat &lt;- dat %&gt;% group_by(Country,Sex,Date,Age,Week) %&gt;% select(Exc) %&gt;% summarize_all(sum) # Adjust date for US: case countrs from two days earlier than excess mortality cases$Date[cases$Date==&quot;2020-05-23&quot; &amp; cases$Country==&quot;USA&quot;] &lt;- &quot;2020-05-25&quot; # Merge with cases dat &lt;- inner_join(dat,cases[,c(&quot;Country&quot;,&quot;Date&quot;,&quot;Age&quot;,&quot;Sex&quot;,&quot;Cases&quot;)]) # Calculate ASFRs dat &lt;- dat %&gt;% mutate(ascfr = Exc / Cases, ascfr = replace_na(ascfr, 0), ascfr = ifelse(is.infinite(ascfr),0,ascfr), ascfr = ifelse(ascfr&gt;1,1,ascfr)) # Decide some reference patterns (here Germany) DE &lt;- dat %&gt;% filter(Country == &quot;Germany&quot;, Sex == &quot;b&quot;, #Date == maxdate) Week == 19) # Decompose DecDE &lt;- as.data.table(dat)[, kitagawa_cfr(DE$Cases, DE$ascfr,Cases,ascfr), by=list(Country,Week, Sex)] # Select only most recent date, both genders combined DecDE &lt;- DecDE %&gt;% filter(Sex==&quot;b&quot;) %&gt;% group_by(Country) %&gt;% filter(Week %in% 19:22) # Drop unnecessary variables DecDE &lt;- DecDE %&gt;% select(Country,Week,CFR2,Diff,AgeComp,RateComp) # Calculate relative contributions DecDE &lt;- DecDE %&gt;% mutate(relAgeDE = abs(AgeComp)/(abs(AgeComp)+abs(RateComp))) DecDE &lt;- DecDE %&gt;% mutate(relRateDE = abs(RateComp)/(abs(AgeComp)+abs(RateComp))) # Rename DecDE &lt;- DecDE %&gt;% rename(DiffDE=Diff,AgeCompDE=AgeComp,RateCompDE=RateComp) # Sort data DecDE &lt;- DecDE %&gt;% arrange(CFR2) # Appendix ### Save extra table ########################################################## # Appendix table 1 write_xlsx(x=DecDE, path=&quot;files/COVID_article/AppendixTab6.xlsx&quot;) # Load data db_gh &lt;- read_csv(&quot;files/COVID_article/inputdata.csv&quot;) ### Aggregate data ############################################################ # Filter date db_gh$Date &lt;- as.Date(db_gh$Date,&quot;%d.%m.%y&quot;) db_gh2 &lt;- db_gh %&gt;% filter(Date&lt;=as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%y&quot;)) # Set New York as &quot;country&quot; (easier handling) db_gh2$Country[db_gh2$Country==&quot;USA&quot; &amp; db_gh2$Region == &quot;NYC&quot;] &lt;- &quot;NYC&quot; # Sum data over age groups db_gh2 &lt;- db_gh2 %&gt;% filter(!Country %in% c(&quot;China&quot;,&quot;USA&quot;,&quot;South Korea&quot;) &amp; Sex == &quot;b&quot;) %&gt;% group_by(Country, Code,Date) %&gt;% summarise(Cases = sum(Cases), Deaths = sum(Deaths)) # Exclude bolletino db_gh2 &lt;- db_gh2 %&gt;% filter(str_sub(Code, 1, 5) != &quot;ITbol&quot;) # Sort by date db_gh2 &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% arrange(Date) # Smooth reporting issues cases for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } # Smooth reporting issues deaths for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } ### Plot settings ############################################################# # Set colors col_country &lt;- c(&quot;Germany&quot; = &quot;black&quot;, &quot;Italy&quot; = &quot;#2ca25f&quot;, &quot;NYC&quot;=&quot;#f0027f&quot;, &quot;Spain&quot;=&quot;#beaed4&quot;, &quot;South Korea&quot;=&quot;#fdc086&quot;)#, #&quot;USA&quot;=&quot;#386cb0&quot;) cols &lt;- c(&quot;black&quot;, &quot;#2ca25f&quot;, &quot;#f0027f&quot;, &quot;#beaed4&quot;, &quot;#fdc086&quot;)#, #&quot;#386cb0&quot;) # Axis labs &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% filter(Cases == max(Cases)) %&gt;% mutate(Cases = Cases + 3000) # Including all reports tx &lt;- 6 lim_x &lt;- 240000 ### Plot ###################################################################### db_gh2 %&gt;% ggplot(aes(Cases, Deaths, col = Country))+ geom_line(size = 1, alpha = .9)+ scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+ scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;text&quot;, label = &quot;2% CFR&quot;, x = lim_x + 1000, y = lim_x * .02, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;5% CFR&quot;, x = lim_x + 1000, y = lim_x * .05, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;10% CFR&quot;, x = lim_x + 1000, y = lim_x * .10, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;15% CFR&quot;, x = lim_x + 1000, y = lim_x * .15, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + scale_colour_manual(values = cols)+ geom_text(data = labs, aes(Cases, Deaths, label = Country), size = tx * .35, hjust = 0, fontface = &quot;bold&quot;) + theme_classic()+ labs(x = &quot;Cases&quot;, y = &quot;Deaths&quot;)+ theme( panel.grid.minor = element_blank(), legend.position = &quot;none&quot;, plot.margin = margin(5,5,5,5,&quot;mm&quot;), axis.text.x = element_text(size = tx), axis.text.y = element_text(size = tx), axis.title.x = element_text(size = tx + 1), axis.title.y = element_text(size = tx + 1) ) # Save ggsave(&quot;files/COVID_article/Fig_1.jpg&quot;, width = 4, height = 3, dpi = 600) References "],["data-management-2.html", "Data management (2)", " Data management (2) During the course, I learned to use the Guerrilla analytics framework to manage my data in a clear way. The Guerrilla analytics framework consists of seven principles, which I will shortly introduce (Gestel, n.d.). Space is cheap, confusion is expensive. Basically saying that storage costs are low. Always keep your files, store them in a reliable online cloud and protect yourself from cyber criminals. Use simple, visual project structures and conventions. Organize files and folders in a system that makes it understandable for other team members. A couple of things to think of is to avoid deep nesting in folders, create a separate folder for each project and use subfolders for different datasets. Also try not to change names or move them, this makes it harder to lose. Automate with program code. This makes reproducibility possible. Link stored data to data in the analytics environment to data in work products. When storing data on different platforms, make shure to link those platforms before anything gets lost. Version control changes to data and analytics code. I used Gihub for version control, to secure all changes and data. Consolidate team knowledge. When working together, make shure to have a good communication line. Use code that runs from start to finish. This also makes it easier to reproduce or revisit later on. An example of applying the Guerrilla analytics framework is linked below. This shows a project of a previous course I followed (DAUR2). Figure 1: Folder tree DAUR2. References "],["free-future-assignment-3.html", "Free future assignment (3) Introduction Plan A phylogenetic analysis", " Free future assignment (3) Introduction After this course i will be specializing in microbiology, this is also where I see myself in a couple of years. Data science for me is a great opportunity to combine different work areas and keep my work various, but my main focus will be microbiology. Before i start working in the field, i hope to earn a masters degree, so in two years i will probably be busy with my masters. Because i am interested in microbiology, my free assignment will also be in that lane. I will be learning to perform a phylogenetic analysis to a COVID dataset in R. Plan To achieve my goal, i have planned 4 days total to work on it. I have divided the days in a couple groups: - Half a day to come up with a plan, write this and search the right data needed. - One day to follow a tutorial. - Two days to implement this tutorial on the data i will be using and complete the analysis. - Half a day to finish the writing and details. A phylogenetic analysis "],["introduction-groupproject-5.html", "Introduction groupproject (5) ONTOX Our addition", " Introduction groupproject (5) ONTOX The ONTOX Consortium is a research initiative dedicated to the study and application of ontological technology to develop solutions for real-world problems. The consortium is made up of leading experts in ontology and semantic technology from around the world. The research team is focused on developing ontology-based applications to address problems in the areas of health, environment, and security. Using ontology-based frameworks, the consortium is developing innovative solutions for data integration and data sharing, as well as for managing and exploiting huge amounts of data and actively exploring the use of ontology-based methods for decision support and artificial intelligence applications. The European Commission is investing in this concept and has recently launched the “ONTOX” project, which is focused on providing a functional and sustainable solution for assessing the human risk of chemicals without animal testing. The project will create new approach methodologies (NAMs) which use computational systems based on artificial intelligence and are fed by biological, toxicological, chemical and kinetic data. These NAMs will be able to predict systemic repeated dose toxicity effects and, when combined with tailored exposure assessment, will allow for human risk assessment. ONTOX is expected to have a long-lasting effect, reinforcing Europe’s role in the development and application of animal-free methods for risk assessment of chemicals (Vinken et al. 2021). As they state themselves: “The vision of the ONTOX consortium is to provide a functional and sustainable solution for advancing human risk assessment of chemicals without the use of animals in line with the principles of 21st century toxicity testing and next generation risk assessment.” (“ONTOX Project,” n.d.) One of the ways to achieve this is via the Phymdos app, created by D. Roodzant for University of Applied Sciences Utrecht. The app collects and combines existing data for risk assessment using the SysRev platform, this platform helps with data curation, SERs (Systematic Evidence Reviews), and managed reviews. It has been used to create thousands of projects, covering a wide range of disciplines whom are all publicly accessible (Bozada et al. 2021). In this case, the platform extracts metabolic pathways and compound interactions from data. The data is then formatted into an SBtab file which can be used to create visualisations or other formats such as SBML. SBtab is a data format designed for Systems Biology that builds from the structure of spreadsheets. It defines conventions for table structure, controlled vocabularies, and semantic annotations to support automated data integration and model building. There are predefined table types for experimental data and SBML-compliant models, and the format can be easily customized to include new types of data (Lubitz et al. 2016). Our addition Our goal is to add to the function off the Phymdos app. The app now only allows to extract and analyze from one single tab, but to apply this process to a larger scope, it should be combining data from multiple sources instead of just focusing on one. So, the assignment we have received from Marc Teunis (The ONTOX project) is to retrieve the SBtab data from multiple articles and to merge these multiple files into one new file. Additionally, we must create a graph to visualize the data and must convert de SBtab file to a SBML format. This will all be combined in a R package (“Project ONTOX: Data Sciences for Biology 2,” n.d.). References "],["relational-data-and-databases-7.html", "Relational data and databases (7) Introduction DBeaver connection Inspection Descriptive statistics Visualisations", " Relational data and databases (7) Introduction This assignment was focused on learning the basics of the SQL language and how to work this around relational data and databases. These are collections of information stored in separate tables with underlying relations to create an easy overview of relations between different data structures. DBeaver connection Start by creating three data frames out of files and making them tidy. library(dslabs) library(tidyverse) library(DBI) library(dplyr) library(ggplot2) library(car) library(FSA) # The flu ## Import, first 11 lines are metadata flu_df &lt;- read_csv(&quot;files/flu_data.csv&quot;, skip = 11) ## Make tidy flu_df_tidy &lt;- flu_df %&gt;% pivot_longer(cols = c(2:ncol(flu_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Dengue ## Import, first 11 lines are metadata dengue_df &lt;- read_csv(&quot;files/dengue_data.csv&quot;, skip = 11) ## Make tidy dengue_df_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = c(2:ncol(dengue_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Gapminder ## The dataframe is part of the dslabs package and therefore does not need to be imported. ## The dataframe is already tidy. An important part of relational data is enabling comparison across different data frames. In this case we will be looking at the country and date, but in the data frames we use there are a couple of things that needs to be adjusted to make them similar. # Column &#39;country&#39; ## Check types. class(flu_df_tidy$country) # Character ## [1] &quot;character&quot; class(dengue_df_tidy$country) # Character ## [1] &quot;character&quot; class(gapminder$country) # Factor ## [1] &quot;factor&quot; ## Change all types into factor flu_df_tidy$country &lt;- as.factor(flu_df_tidy$country) dengue_df_tidy$country &lt;- as.factor(dengue_df_tidy$country) ## Check if it worked class(flu_df_tidy$country) # Factor ## [1] &quot;factor&quot; class(dengue_df_tidy$country) # Factor ## [1] &quot;factor&quot; ## It worked! # Column &#39;date&#39; ## Check types. class(flu_df_tidy$Date) # Date ## [1] &quot;Date&quot; class(dengue_df_tidy$Date) # Date ## [1] &quot;Date&quot; class(gapminder$year) # Integer ## [1] &quot;integer&quot; ## Because flu and dengue are specified to day instead of the year at gapminder, a &#39;year&#39; column needs to be added and changed into the integer type to create similarity. flu_df_tidy &lt;- flu_df_tidy %&gt;% mutate(year = substr(flu_df_tidy$Date, start=1, stop=4)) dengue_df_tidy &lt;- dengue_df_tidy %&gt;% mutate(year = substr(dengue_df_tidy$Date, start=1, stop=4)) flu_df_tidy$year &lt;- as.integer(flu_df_tidy$year) dengue_df_tidy$year &lt;- as.integer(dengue_df_tidy$year) ## Check if it worked. class(flu_df_tidy$year) # Integer ## [1] &quot;integer&quot; class(dengue_df_tidy$year) # Integer ## [1] &quot;integer&quot; ## It worked! The data frames are now similar and can be stored into csv and rds files. # Export data frames as csv write.csv(flu_df_tidy, &quot;files/flu.csv&quot;, row.names=FALSE) write.csv(dengue_df_tidy, &quot;files/dengue.csv&quot;, row.names=FALSE) write.csv(gapminder, &quot;files/gapminder.csv&quot;, row.names=FALSE) # Export data frames as rds saveRDS(flu_df_tidy, file=&quot;files/flu.rds&quot;) saveRDS(dengue_df_tidy, file=&quot;files/dengue.rds&quot;) saveRDS(gapminder, file=&quot;files/gapminder.rds&quot;) DBeaver and R were connected (my real password is hidden) and the tables were inserted into the ‘workflowsdb’ database in DBeaver for inspection. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;Password&quot;) # Hide my real password dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbWriteTable(con, &quot;flu&quot;, flu_df_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_df_tidy) Inspection Now that the tables are imported, i checked a couple of things. Dengue has no “NULL” variables in country and year, 6263 cases in the data set and the average amount of cases is 0.14 with a standard deviation of 0.14. The country with the least amount of cases is Argentina with 0 cases and the country with the most is Venezuela with 1 case. The flu has no “NULL variables in country and year, 17266 cases in the data set and the average amount of cases is 473.74 with a standard deviation of 768.95. The country with the least amount of cases is Argentina with 0 cases and the country with the most is Uruguay 10555 cases. Gapminder had no “NULL” variables in country and year. There was an important thing that stood out to me about the metadata. I will discuss this later in this chapter. The information is obtained by the following SQL and R code: Figure 2: SQL code of data inspection. # Dengue min(dengue_df_tidy$cases) # 0 ## [1] 0 max(dengue_df_tidy$cases) # 1 ## [1] 1 mean(dengue_df_tidy$cases) # 0.14 ## [1] 0.1389711 sd(dengue_df_tidy$cases) # 0.14 ## [1] 0.1389235 # Flu min(flu_df_tidy$cases) # 0 ## [1] 0 max(flu_df_tidy$cases) # 10555 ## [1] 10555 mean(flu_df_tidy$cases) # 473.74 ## [1] 473.7355 sd(flu_df_tidy$cases) # 768.95 ## [1] 768.9549 For a good analysis of the different, but related data, they need to be joined together in one table with the important columns. I created four joined tables, flu with dengue, dengue with gapminder, flu with gapminder and all three of them. Some tables have more countries than others, this is because the joining only joins the matching countries. See below for the SQL and R code. Figure 3: SQL code of inner-join. # Load joined table to object gapminder_flu &lt;- read_csv(&quot;files/gapminder_flu.csv&quot;) gapminder_dengue &lt;- read_csv(&quot;files/gapminder_dengue.csv&quot;) flu_dengue &lt;- read_csv(&quot;files/flu_dengue.csv&quot;) gapminder_flu_dengue &lt;- read_csv(&quot;files/gapminder_flu_dengue.csv&quot;) As said before, there was something important that stood out to me, the dataset missed metadata. The cases columns didn’t specify how it was messured, and this was also not written down in the metadata. This resulted in unreliable data that doesn’t take in account that countries have different populations, so just a count of cases isn’t enough. Therefore a normalisation was necessary. I divided the cases by the population from the gapminder data and multiplied this by 100000, which seemed like a logical amount when calculating with populations. I added the results to the tables. gapminder_flu &lt;- gapminder_flu %&gt;% mutate(&quot;cases_per_100000&quot; =gapminder_flu$flu_cases/gapminder_flu$population*100000) gapminder_dengue &lt;- gapminder_dengue %&gt;% mutate(&quot;cases_per_100000&quot; =gapminder_dengue$dengue_cases/gapminder_dengue$population*100000) Descriptive statistics Now that the sets are normalized, descriptives can start. Starting by calculating the means and standard deviations of cases per country with the updated data. After performing the Shapiro-Wilk test, the flu seems to have 8 non normal distributed variables from a total of 29, and dengue has 5 non normal distributed variables from a total of 10. As an extra test, the Levene’s test shows that the flu doesn’t have a variance of equility between countries. Because of these two specifics, a Kruskal-Wallis test was chosen, because this test doesn’t account for normality. The results of this test shows that both datasets have a p-value of &lt; 2.2e-16 which tells us that there is a significance difference. To determine the differences between variables, the Dunn test was performed as a post hoc test. The results of those are stored in separate tables of difference and no difference, but in total 114 from a total of 406 combinations of the flu show differences and 19 from a total of 45 combinations of dengue show differences. # Obtain mean and standard deviation per country flu_sum &lt;- gapminder_flu %&gt;% group_by(country)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) dengue_sum &lt;- gapminder_dengue %&gt;% group_by(country)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) # To check the normality, dataframes are made with the countries flu_country &lt;- gapminder_flu %&gt;% select(country, cases_per_100000, year) %&gt;% pivot_wider(names_from = country, values_from = cases_per_100000) %&gt;% filter(year != 2002) dengue_country &lt;- gapminder_dengue %&gt;% select(country, cases_per_100000, year) %&gt;% pivot_wider(names_from = country, values_from = cases_per_100000) %&gt;% filter(year != 2002) # Shapiro wilk ## Flu flu_stats &lt;- data.frame(country = colnames(flu_country[,2:length(colnames(flu_country))]), pvalue_shap = round(c(shapiro.test(flu_country$Uruguay)$p.value, shapiro.test(flu_country$`United States`)$p.value, shapiro.test(flu_country$Ukraine)$p.value, shapiro.test(flu_country$Switzerland)$p.value, shapiro.test(flu_country$Sweden)$p.value, shapiro.test(flu_country$Spain)$p.value, shapiro.test(flu_country$`South Africa`)$p.value, shapiro.test(flu_country$Russia)$p.value, shapiro.test(flu_country$Romania)$p.value, shapiro.test(flu_country$Poland)$p.value, shapiro.test(flu_country$Peru)$p.value, shapiro.test(flu_country$Paraguay)$p.value, shapiro.test(flu_country$Norway)$p.value, shapiro.test(flu_country$`New Zealand`)$p.value, shapiro.test(flu_country$Netherlands)$p.value, shapiro.test(flu_country$Mexico)$p.value, shapiro.test(flu_country$Japan)$p.value, shapiro.test(flu_country$Hungary)$p.value, shapiro.test(flu_country$Germany)$p.value, shapiro.test(flu_country$France)$p.value, shapiro.test(flu_country$Chile)$p.value, shapiro.test(flu_country$Canada)$p.value, shapiro.test(flu_country$Bulgaria)$p.value, shapiro.test(flu_country$Brazil)$p.value, shapiro.test(flu_country$Bolivia)$p.value, shapiro.test(flu_country$Belgium)$p.value, shapiro.test(flu_country$Austria)$p.value, shapiro.test(flu_country$Australia)$p.value, shapiro.test(flu_country$Argentina)$p.value), 4)) ## Dengue dengue_stats &lt;- data.frame(country = colnames(dengue_country[,2:length(colnames(dengue_country))]), pvalue_shap = round(c(shapiro.test(dengue_country$Venezuela)$p.value, shapiro.test(dengue_country$Thailand)$p.value, shapiro.test(dengue_country$Singapore)$p.value, shapiro.test(dengue_country$Philippines)$p.value, shapiro.test(dengue_country$Mexico)$p.value, shapiro.test(dengue_country$Indonesia)$p.value, shapiro.test(dengue_country$India)$p.value, shapiro.test(dengue_country$Brazil)$p.value, shapiro.test(dengue_country$Bolivia)$p.value, shapiro.test(dengue_country$Argentina)$p.value), 4)) # Normality check flu_stats &lt;- flu_stats %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) # 3 out of 5 are normally distributed dengue_stats &lt;- dengue_stats %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) # 21 out of 29 are normally distributed # Levene test leveneTest(cases_per_100000 ~ country, data = gapminder_flu) # &lt;2.2e-16 ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 28 12.482 &lt; 2.2e-16 *** ## 330 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 leveneTest(cases_per_100000 ~ country, data = gapminder_dengue) # 3.228e-07 ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 9 6.2485 3.288e-07 *** ## 120 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Kruskal Wallis test kruskal.test(cases_per_100000 ~ country, data = gapminder_flu) # p-value &lt; 2.2e-16 ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by country ## Kruskal-Wallis chi-squared = 332.13, df = 28, p-value &lt; 2.2e-16 kruskal.test(cases_per_100000 ~ country, data = gapminder_dengue) # p-value &lt; 2.2e-16 ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by country ## Kruskal-Wallis chi-squared = 110.54, df = 9, p-value &lt; 2.2e-16 # P-value of basically .0000, there is a statistically significant difference # Dunn post-hoc test dengue_dunn &lt;- dunnTest(cases_per_100000 ~ country, data = gapminder_dengue, method = &quot;holm&quot;)$res flu_dunn &lt;- dunnTest(cases_per_100000 ~ country, data = gapminder_flu, method = &quot;holm&quot;)$res # Check for significant differences dengue_dunn &lt;- dengue_dunn %&gt;% mutate(different = P.adj&lt;0.05) flu_dunn &lt;- flu_dunn %&gt;% mutate(different = P.adj&lt;0.05) # Note the significant differences between countries/continents dengue_dunn_diff &lt;- dengue_dunn %&gt;% filter(different == TRUE) flu_dunn_diff &lt;- flu_dunn %&gt;% filter(different == TRUE) # Note the comparable countries/continents dengue_dunn_non &lt;- dengue_dunn %&gt;% filter(different == FALSE) flu_dunn_non &lt;- flu_dunn %&gt;% filter(different == FALSE) Visualisations Flu gapminder_flu %&gt;% ggplot(aes(x=continent,y=cases_per_100000, group=continent, fill=continent)) + geom_col(stat=&quot;identity&quot;)+ labs(title = &quot;Flu cases per continent&quot;, x=&quot;Continent&quot;, y=&quot;Flu cases per 100000 residents &quot;, caption = &quot;Figure 3: Bargraph of the amount of flu cases per continent&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) ggplot(flu_dengue, aes(x=year, y=flu_cases, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Flu cases&quot;, x = &quot;Year&quot;, y = &quot;Average dengue cases per year&quot;, caption = &quot;Figure 4: Linegraph of average flu cases per year per country.&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) Dengue gapminder_dengue %&gt;% ggplot(aes(x=continent,y=cases_per_100000, group=continent, fill=continent)) + geom_col(stat=&quot;identity&quot;)+ labs(title = &quot;Dengue cases per continent&quot;, x=&quot;Continent&quot;, y=&quot;Dengue cases per 100000 residents&quot;, caption = &quot;Figure 5: Bargraph of the amount of dengue cases per continent&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) ggplot(flu_dengue, aes(x=year, y=dengue_cases, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Dengue cases&quot;, x = &quot;Year&quot;, y = &quot;Average dengue cases per year&quot;, caption = &quot;Figure 6: Linegraph of average dengue cases per year per country.&quot;) + theme(legend.position = &quot;right&quot;, text = element_text(size=10)) "],["r-packages-8.html", "R packages (8) Introduction Copopa", " R packages (8) Introduction In this assignment I created a R package to support my portfolio, following The Whole Game demo made by Hadley Wickham. The demo guided me through setting up my own package and helped me create something useful on my own after. Copopa To start a new package, a name is necessary. I combined my name, “portfolio” and “package” together into a fun word an checked availability with the “available” package. The name was available, so I set up the project and the github repository to get started. A package to support my portfolio would reduce duplicated code. But scanning through my portfolio, I noticed that there wasn’t a lot of code or duplicated code. Luckily, i found some in the “Relational data and databases” assignment that would work for this. The package that i made contains two functions: check_type() -&gt; checks the data type of datasets, or columns in it. save_to_csv_rds() -&gt; saves a dataframe into CSV and RDS files. For the manual and contents of the package, please visit the copopa github repository. "],["rmarkdown-parameters-9.html", "Rmarkdown parameters (9)", " Rmarkdown parameters (9) blabla "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
